{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6bToUx6GD4P"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G0tGPoUzGFib"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import combinations\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcsCzy3Sw1rO"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LBvFi361upRz"
      },
      "outputs": [],
      "source": [
        "class WindowAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = (dim // heads) ** -0.5\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, N, C)\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(\n",
        "            lambda t: t.view(B, N, self.heads, C // self.heads).transpose(1, 2),\n",
        "            qkv\n",
        "        )  # (B, heads, N, dim_head)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # (B, heads, N, N)\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = torch.matmul(attn, v)  # (B, heads, N, dim_head)\n",
        "        out = out.transpose(1, 2).reshape(B, N, C)  # (B, N, C)\n",
        "        out = self.to_out(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A4xlq64busm7"
      },
      "outputs": [],
      "source": [
        "class MSFF_WinAttn_MobileNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        mobilenet = models.mobilenet_v2(pretrained=True).features\n",
        "\n",
        "        # Feature stages\n",
        "        self.stage1 = mobilenet[:4]    # 24-d\n",
        "        self.stage2 = mobilenet[4:7]   # 32-d\n",
        "        self.stage3 = mobilenet[7:14]  # 96-d\n",
        "        self.stage4 = mobilenet[14:]   # 1280-d\n",
        "\n",
        "        # Reduce channels to 256 for fusion\n",
        "        self.reduce1 = nn.Conv2d(24, 256, 1)\n",
        "        self.reduce2 = nn.Conv2d(32, 256, 1)\n",
        "        self.reduce3 = nn.Conv2d(96, 256, 1)\n",
        "        self.reduce4 = nn.Conv2d(1280, 256, 1)\n",
        "\n",
        "        # Window attention with LayerNorm before and after\n",
        "        self.win_attn = nn.Sequential(\n",
        "            nn.LayerNorm(256),\n",
        "            WindowAttention(dim=256, heads=4),\n",
        "            nn.LayerNorm(256)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(256 * 4, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feature extraction\n",
        "        x1 = self.stage1(x)\n",
        "        x2 = self.stage2(x1)\n",
        "        x3 = self.stage3(x2)\n",
        "        x4 = self.stage4(x3)\n",
        "\n",
        "        # Reduce + GAP\n",
        "        x1 = F.adaptive_avg_pool2d(self.reduce1(x1), 1).flatten(1)\n",
        "        x2 = F.adaptive_avg_pool2d(self.reduce2(x2), 1).flatten(1)\n",
        "        x3 = F.adaptive_avg_pool2d(self.reduce3(x3), 1).flatten(1)\n",
        "        x4 = F.adaptive_avg_pool2d(self.reduce4(x4), 1).flatten(1)\n",
        "\n",
        "        # Stack multi-scale features\n",
        "        feats = torch.stack([x1, x2, x3, x4], dim=1)  # (B, 4, 256)\n",
        "\n",
        "        # Apply window attention\n",
        "        feats = self.win_attn(feats)\n",
        "\n",
        "        # Flatten and classify\n",
        "        out = feats.flatten(1)  # (B, 4*256)\n",
        "        return self.classifier(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nExLMKfuvpy",
        "outputId": "18d6238f-9fd0-4c9a-c3b4-3520278f8a43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 114MB/s] \n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MSFF_WinAttn_MobileNet(num_classes=2).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvdG0jB6xbkV"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DxY8CSD9t_DT"
      },
      "outputs": [],
      "source": [
        "def get_albumentations_test_transform():\n",
        "    return A.Compose([\n",
        "        A.Resize(224, 224),\n",
        "        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "class AlbumentationsDataset(ImageFolder):\n",
        "    def __init__(self, root, transform=None):\n",
        "        super().__init__(root)\n",
        "        self.albumentations_transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, label = super().__getitem__(index)\n",
        "        image = np.array(image)\n",
        "        if self.albumentations_transform:\n",
        "            image = self.albumentations_transform(image=image)['image']\n",
        "        return image, label\n",
        "\n",
        "def test_model(model, model_path, test_folder, device='cuda' if torch.cuda.is_available() else 'cpu', batch_size=32):\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_transform = get_albumentations_test_transform()\n",
        "    test_dataset = AlbumentationsDataset(test_folder, transform=test_transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    rec = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"\\n Test Accuracy: {acc:.4f}\")\n",
        "    print(f\" Precision:     {prec:.4f}\")\n",
        "    print(f\" Recall:        {rec:.4f}\")\n",
        "    print(f\" F1 Score:      {f1:.4f}\")\n",
        "\n",
        "    return acc, prec, rec, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjfD82gRuXgM",
        "outputId": "189ad939-83c4-4809-cc11-f7304c42868e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Test Accuracy: 0.9479\n",
            " Precision:     0.9293\n",
            " Recall:        0.8949\n",
            " F1 Score:      0.9108\n"
          ]
        }
      ],
      "source": [
        "test_folder = \"/content/extracted_folder/Comys_Hackathon5/Task_A/val\"\n",
        "model_path = \"/content/best_model_Task_A.pth\"\n",
        "\n",
        "acc, prec, rec, f1 = test_model(model, model_path, test_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiUqUyS5uj10",
        "outputId": "be8e9749-7f65-4054-ab07-6ec2d69101ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Test Accuracy: 0.9709\n",
            " Precision:     0.9383\n",
            " Recall:        0.9546\n",
            " F1 Score:      0.9462\n"
          ]
        }
      ],
      "source": [
        "test_folder = \"/content/extracted_folder/Comys_Hackathon5/Task_A/train\"\n",
        "model_path = \"/content/best_model_Task_A.pth\"\n",
        "\n",
        "acc, prec, rec, f1 = test_model(model, model_path, test_folder)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
