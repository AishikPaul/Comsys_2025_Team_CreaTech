{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6bToUx6GD4P"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G0tGPoUzGFib"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import combinations\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcsCzy3Sw1rO"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sDUKvzeWvMKC"
      },
      "outputs": [],
      "source": [
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob=0.):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.drop_prob == 0. or not self.training:\n",
        "            return x\n",
        "        keep_prob = 1 - self.drop_prob\n",
        "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "        random_tensor.floor_()\n",
        "        return x.div(keep_prob) * random_tensor\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = (dim // heads) ** -0.5\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(\n",
        "            lambda t: t.view(B, N, self.heads, C // self.heads).transpose(1, 2),\n",
        "            qkv\n",
        "        )\n",
        "        dots = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "        attn = dots.softmax(dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).reshape(B, N, C)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class MSFF_WinAttn_MobileNet_Embedding(nn.Module):\n",
        "    def __init__(self, embedding_dim=128, drop_path_prob=0.1):\n",
        "        super().__init__()\n",
        "        mobilenet = models.mobilenet_v2(pretrained=True).features\n",
        "\n",
        "        self.stage1 = mobilenet[:4]\n",
        "        self.stage2 = mobilenet[4:7]\n",
        "        self.stage3 = mobilenet[7:14]\n",
        "        self.stage4 = mobilenet[14:]\n",
        "\n",
        "        self.reduce1 = nn.Conv2d(24, 256, 1)\n",
        "        self.reduce2 = nn.Conv2d(32, 256, 1)\n",
        "        self.reduce3 = nn.Conv2d(96, 256, 1)\n",
        "        self.reduce4 = nn.Conv2d(1280, 256, 1)\n",
        "\n",
        "        self.attn1 = WindowAttention(256, heads=4)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.drop_path1 = DropPath(drop_path_prob)\n",
        "\n",
        "        self.attn2 = WindowAttention(256, heads=4)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.drop_path2 = DropPath(drop_path_prob)\n",
        "\n",
        "        self.embed_fc = nn.Linear(256 * 4, embedding_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.bn_final = nn.BatchNorm1d(256 * 4)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.stage1(x)\n",
        "        x2 = self.stage2(x1)\n",
        "        x3 = self.stage3(x2)\n",
        "        x4 = self.stage4(x3)\n",
        "\n",
        "        x1 = self.dropout(F.adaptive_avg_pool2d(self.reduce1(x1), 1).flatten(1))\n",
        "        x2 = self.dropout(F.adaptive_avg_pool2d(self.reduce2(x2), 1).flatten(1))\n",
        "        x3 = self.dropout(F.adaptive_avg_pool2d(self.reduce3(x3), 1).flatten(1))\n",
        "        x4 = self.dropout(F.adaptive_avg_pool2d(self.reduce4(x4), 1).flatten(1))\n",
        "\n",
        "\n",
        "        feats = torch.stack([x1, x2, x3, x4], dim=1)\n",
        "        feats = self.attn1(feats)\n",
        "        feats = self.drop_path1(feats)\n",
        "        B, N, C = feats.shape\n",
        "        feats = feats.view(B * N, C)\n",
        "        feats = self.bn1(feats)\n",
        "        feats = F.relu(feats)\n",
        "        feats = feats.view(B, N, C)\n",
        "\n",
        "        feats = self.attn2(feats)\n",
        "        feats = self.drop_path2(feats)\n",
        "        feats = feats.view(B * N, C)\n",
        "        feats = self.bn2(feats)\n",
        "        feats = F.relu(feats)\n",
        "        feats = feats.view(B, N, C)\n",
        "\n",
        "        out = feats.flatten(1)\n",
        "        # embed = self.embed_fc(out)\n",
        "        out = self.dropout(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.bn_final(out)  # <-- Add in __init__\n",
        "        embed = self.embed_fc(out)\n",
        "\n",
        "        embed = F.normalize(embed, p=2, dim=1)\n",
        "        return embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzRQzg5dvM4E",
        "outputId": "32017348-f62e-4184-8c1f-6fdd37161351"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 42.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MSFF_WinAttn_MobileNet_Embedding(embedding_dim=128).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqMdzZ2bygpV"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NRAduTgAw36w"
      },
      "outputs": [],
      "source": [
        "def load_images_from_folder(folder, image_size):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img_path = os.path.join(folder, filename)\n",
        "        if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):  # Ensure only images\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is not None:\n",
        "                img = cv2.resize(img, image_size)\n",
        "                images.append(img)\n",
        "    return images\n",
        "\n",
        "def create_pairs_with_distortions_balanced(base_path, save_dir, image_size=(128, 128)):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(save_dir, \"x1\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(save_dir, \"x2\"), exist_ok=True)\n",
        "\n",
        "    people = [p for p in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, p))]\n",
        "    pair_records = []\n",
        "    pair_id = 0\n",
        "    positive_pairs = []\n",
        "\n",
        "    print(\"Generating positive pairs...\")\n",
        "\n",
        "    # Collect all positive pairs\n",
        "    person_to_images = {}\n",
        "    for person in tqdm(people):\n",
        "        person_path = os.path.join(base_path, person)\n",
        "        normal_images = load_images_from_folder(person_path, image_size)\n",
        "        distorted_path = os.path.join(person_path, \"distorted\")\n",
        "        distorted_images = []\n",
        "        if os.path.exists(distorted_path):\n",
        "            distorted_images = load_images_from_folder(distorted_path, image_size)\n",
        "\n",
        "        all_images = normal_images + distorted_images\n",
        "        person_to_images[person] = all_images\n",
        "\n",
        "        for img1, img2 in combinations(all_images, 2):\n",
        "            positive_pairs.append((img1, img2))\n",
        "\n",
        "    num_positive = len(positive_pairs)\n",
        "    print(f\"Total positive pairs: {num_positive}\")\n",
        "\n",
        "    #Save positive pairs\n",
        "    for img1, img2 in positive_pairs:\n",
        "        x1_path = os.path.join(\"x1\", f\"pair_{pair_id}.jpg\")\n",
        "        x2_path = os.path.join(\"x2\", f\"pair_{pair_id}.jpg\")\n",
        "        cv2.imwrite(os.path.join(save_dir, x1_path), img1)\n",
        "        cv2.imwrite(os.path.join(save_dir, x2_path), img2)\n",
        "        pair_records.append([x1_path, x2_path, 1])\n",
        "        pair_id += 1\n",
        "\n",
        "    # Generate negative pairs equal in number to positive pairs\n",
        "    print(\"Generating balanced negative pairs...\")\n",
        "    negative_pairs = []\n",
        "    attempts = 0\n",
        "    max_attempts = num_positive * 10  # safety cap\n",
        "\n",
        "    while len(negative_pairs) < num_positive and attempts < max_attempts:\n",
        "        person1, person2 = random.sample(people, 2)\n",
        "        imgs1 = person_to_images[person1]\n",
        "        imgs2 = person_to_images[person2]\n",
        "\n",
        "        if not imgs1 or not imgs2:\n",
        "            attempts += 1\n",
        "            continue\n",
        "\n",
        "        img1 = random.choice(imgs1)\n",
        "        img2 = random.choice(imgs2)\n",
        "\n",
        "        negative_pairs.append((img1, img2))\n",
        "        attempts += 1\n",
        "\n",
        "\n",
        "    for img1, img2 in negative_pairs:\n",
        "        x1_path = os.path.join(\"x1\", f\"pair_{pair_id}.jpg\")\n",
        "        x2_path = os.path.join(\"x2\", f\"pair_{pair_id}.jpg\")\n",
        "        cv2.imwrite(os.path.join(save_dir, x1_path), img1)\n",
        "        cv2.imwrite(os.path.join(save_dir, x2_path), img2)\n",
        "        pair_records.append([x1_path, x2_path, 0])\n",
        "        pair_id += 1\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(pair_records, columns=[\"img1\", \"img2\", \"label\"])\n",
        "    df.to_csv(os.path.join(save_dir, \"pairs_labels.csv\"), index=False)\n",
        "    print(f\"Total pairs saved: {len(df)} (Positive: {df['label'].sum()}, Negative: {(df['label']==0).sum()})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ns2jLmm6wyDA"
      },
      "outputs": [],
      "source": [
        "def test_model(model, model_path, test_folder, device='cuda' if torch.cuda.is_available() else 'cpu', batch_size=32):\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    create_pairs_with_distortions_balanced(test_folder,\"./test_pairs\")\n",
        "    test_dataset = FacePairsDataset(\"./test_pairs/pairs_labels.csv\",\"./test_pairs\",image_size=(128, 128),augment=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    val_loss = 0.0\n",
        "    test_batches = tqdm(test_loader, desc=\"test\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img1, img2, labels in test_batches:\n",
        "            img1 = img1.cuda()\n",
        "            img2 = img2.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            emb1 = model(img1)\n",
        "            emb2 = model(img2)\n",
        "\n",
        "            loss = criterion(emb1, emb2, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            dists = F.pairwise_distance(emb1, emb2)\n",
        "            preds = (dists < threshold).long()\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    rec = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"\\n Test Accuracy: {acc:.4f}\")\n",
        "    print(f\" Precision:     {prec:.4f}\")\n",
        "    print(f\" Recall:        {rec:.4f}\")\n",
        "    print(f\" F1 Score:      {f1:.4f}\")\n",
        "\n",
        "    return acc, prec, rec, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KqJGW1YtxZOA"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torchvision import transforms\n",
        "\n",
        "class FacePairsDataset(Dataset):\n",
        "    def __init__(self, csv_path, base_dir, image_size=(128,128), augment=False):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.base_dir = base_dir\n",
        "        self.image_size = image_size\n",
        "        self.augment = augment\n",
        "\n",
        "        self.transform = A.Compose([\n",
        "            A.Resize(*image_size),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.Rotate(limit=15, p=0.5),\n",
        "            A.Normalize(),\n",
        "            ToTensorV2()\n",
        "                ]) if augment else A.Compose([\n",
        "                    A.Resize(*image_size),\n",
        "                    A.Normalize(),\n",
        "                    ToTensorV2()\n",
        "                ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img1_path = os.path.join(self.base_dir, row['img1'])\n",
        "        img2_path = os.path.join(self.base_dir, row['img2'])\n",
        "\n",
        "        img1 = cv2.imread(img1_path)\n",
        "        img2 = cv2.imread(img2_path)\n",
        "\n",
        "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
        "        img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        img1 = self.transform(image=img1)['image']\n",
        "        img2 = self.transform(image=img2)['image']\n",
        "\n",
        "        label = torch.tensor(row['label'], dtype=torch.float32)\n",
        "\n",
        "        return img1, img2, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jhy-zntey3fQ"
      },
      "outputs": [],
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super().__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, emb1, emb2, label):\n",
        "        dist = F.pairwise_distance(emb1, emb2)\n",
        "        loss_same = label * dist.pow(2)\n",
        "        loss_diff = (1 - label) * F.relu(self.margin - dist).pow(2)\n",
        "        return 0.5 * (loss_same + loss_diff).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rSorrtvyy5QX"
      },
      "outputs": [],
      "source": [
        "criterion = ContrastiveLoss(margin=1.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3A1GLsEJzIc_"
      },
      "outputs": [],
      "source": [
        "threshold = 0.5  # Distance threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLAsjP7jvZ7y",
        "outputId": "b15342e1-9b8a-4a23-96e1-2fe114b904e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating positive pairs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 877/877 [00:06<00:00, 138.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total positive pairs: 34016\n",
            "Generating balanced negative pairs...\n",
            "Total pairs saved: 68032 (Positive: 34016, Negative: 34016)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Test Accuracy: 0.9756\n",
            " Precision:     0.9765\n",
            " Recall:        0.9756\n",
            " F1 Score:      0.9756\n"
          ]
        }
      ],
      "source": [
        "test_folder = \"/content/extracted_folder/Comys_Hackathon5/Task_B/train\"\n",
        "model_path = \"/content/best_model_Task_B.pth\"\n",
        "\n",
        "acc, prec, rec, f1 = test_model(model, model_path, test_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-30T04:52:27.141413Z",
          "iopub.status.busy": "2025-06-30T04:52:27.140963Z",
          "iopub.status.idle": "2025-06-30T04:52:32.328519Z",
          "shell.execute_reply": "2025-06-30T04:52:32.327464Z"
        },
        "id": "a333e302",
        "outputId": "d7449164-d5ff-44b3-850f-925563f84e03",
        "papermill": {
          "duration": 10.039458,
          "end_time": "2025-06-30T04:52:32.329776",
          "exception": false,
          "start_time": "2025-06-30T04:52:22.290318",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating positive pairs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 250/250 [00:02<00:00, 90.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total positive pairs: 619\n",
            "Generating balanced negative pairs...\n",
            "Total pairs saved: 1238 (Positive: 619, Negative: 619)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                     "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Test Accuracy: 0.7593\n",
            " Precision:     0.7850\n",
            " Recall:        0.7593\n",
            " F1 Score:      0.7537\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "test_folder = \"/content/extracted_folder/Comys_Hackathon5/Task_B/val\"\n",
        "model_path = \"/content/best_model_Task_B.pth\"\n",
        "\n",
        "acc, prec, rec, f1 = test_model(model, model_path, test_folder)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
