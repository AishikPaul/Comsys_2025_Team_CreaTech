{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a7ee7544",
      "metadata": {
        "id": "a7ee7544",
        "papermill": {
          "duration": 0.005773,
          "end_time": "2025-06-29T19:10:58.497524",
          "exception": false,
          "start_time": "2025-06-29T19:10:58.491751",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78fc342b",
      "metadata": {
        "id": "78fc342b",
        "papermill": {
          "duration": 0.003713,
          "end_time": "2025-06-29T19:10:58.505454",
          "exception": false,
          "start_time": "2025-06-29T19:10:58.501741",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2721155",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-29T19:10:58.513612Z",
          "iopub.status.busy": "2025-06-29T19:10:58.513323Z",
          "iopub.status.idle": "2025-06-29T19:11:12.100179Z",
          "shell.execute_reply": "2025-06-29T19:11:12.099579Z"
        },
        "id": "e2721155",
        "papermill": {
          "duration": 13.592738,
          "end_time": "2025-06-29T19:11:12.101717",
          "exception": false,
          "start_time": "2025-06-29T19:10:58.508979",
          "status": "completed"
        },
        "tags": [],
        "outputId": "93de4d41-f2c0-4202-a4a4-4a399a1dbfd7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torchvision.datasets import ImageFolder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "468340fb",
      "metadata": {
        "id": "468340fb",
        "papermill": {
          "duration": 0.003459,
          "end_time": "2025-06-29T19:11:12.109111",
          "exception": false,
          "start_time": "2025-06-29T19:11:12.105652",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e1b8585",
      "metadata": {
        "id": "8e1b8585",
        "papermill": {
          "duration": 0.003328,
          "end_time": "2025-06-29T19:11:12.115736",
          "exception": false,
          "start_time": "2025-06-29T19:11:12.112408",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "**Augmentations Applied**\n",
        "\n",
        "- Resize to 224×224 pixels  \n",
        "- Random horizontal flip (50% chance)  \n",
        "- Random rotation (up to ±15°, 50% chance)  \n",
        "- Apply motion blur (30% chance)  \n",
        "- Adjust brightness and contrast randomly (50% chance)  \n",
        "- Add fog effect (30% chance)  \n",
        "- Add rain effect (30% chance)  \n",
        "- Add shadow effect (30% chance)  \n",
        "- Add sun flare effect (20% chance)  \n",
        "- Add Gaussian noise (30% chance)  \n",
        "- Apply CLAHE (Contrast Limited Adaptive Histogram Equalization, 20% chance)  \n",
        "- Normalize image (mean: 0.5, std: 0.5 for each channel)  \n",
        "- Convert image to tensor for model input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a510a2eb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-29T19:11:12.123818Z",
          "iopub.status.busy": "2025-06-29T19:11:12.123054Z",
          "iopub.status.idle": "2025-06-29T19:11:16.421668Z",
          "shell.execute_reply": "2025-06-29T19:11:16.420783Z"
        },
        "id": "a510a2eb",
        "papermill": {
          "duration": 4.303926,
          "end_time": "2025-06-29T19:11:16.423174",
          "exception": false,
          "start_time": "2025-06-29T19:11:12.119248",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Wrapper to apply Albumentations transforms\n",
        "class AlbumentationsTransform:\n",
        "    def __init__(self, transform):\n",
        "        self.transform = transform\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img = np.array(img)\n",
        "        augmented = self.transform(image=img)\n",
        "        return augmented['image']\n",
        "\n",
        "# training data augmentation\n",
        "train_transform = AlbumentationsTransform(A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Rotate(limit=15, p=0.5),\n",
        "    A.MotionBlur(blur_limit=7, p=0.3),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=0.5),\n",
        "    A.RandomFog(p=0.3),\n",
        "    A.RandomRain(p=0.3),\n",
        "    A.RandomShadow(p=0.3),\n",
        "    A.RandomSunFlare(p=0.2),\n",
        "    A.GaussNoise(p=0.3),\n",
        "    A.CLAHE(p=0.2),\n",
        "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "]))\n",
        "\n",
        "# validation data\n",
        "val_transform = AlbumentationsTransform(A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "]))\n",
        "\n",
        "\n",
        "train_dir = '/kaggle/input/comys-hackathon5-2025/Comys_Hackathon5/Task_A/train'\n",
        "val_dir = '/kaggle/input/comys-hackathon5-2025/Comys_Hackathon5/Task_A/val'\n",
        "\n",
        "train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "val_data = datasets.ImageFolder(val_dir, transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db4f2444",
      "metadata": {
        "id": "db4f2444",
        "papermill": {
          "duration": 0.003366,
          "end_time": "2025-06-29T19:11:16.430472",
          "exception": false,
          "start_time": "2025-06-29T19:11:16.427106",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94f25676",
      "metadata": {
        "id": "94f25676",
        "papermill": {
          "duration": 0.003236,
          "end_time": "2025-06-29T19:11:16.437266",
          "exception": false,
          "start_time": "2025-06-29T19:11:16.434030",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## WindowAttention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b29bb9f",
      "metadata": {
        "id": "6b29bb9f",
        "papermill": {
          "duration": 0.003203,
          "end_time": "2025-06-29T19:11:16.443868",
          "exception": false,
          "start_time": "2025-06-29T19:11:16.440665",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "\n",
        "*   Window attention explicitly attends within local windows (e.g.,7×7 patches).\n",
        "\n",
        "*   This allows the model to learn richer relationships among local features,\n",
        "such as:\n",
        "      \n",
        "     ---Part configurations (e.g., corners, edges)\n",
        "\n",
        "     ---Short-range dependencies that convolutions might not fully capture.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe7b364",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-29T19:11:16.451586Z",
          "iopub.status.busy": "2025-06-29T19:11:16.451344Z",
          "iopub.status.idle": "2025-06-29T19:11:16.457915Z",
          "shell.execute_reply": "2025-06-29T19:11:16.457128Z"
        },
        "id": "0fe7b364",
        "papermill": {
          "duration": 0.012034,
          "end_time": "2025-06-29T19:11:16.459323",
          "exception": false,
          "start_time": "2025-06-29T19:11:16.447289",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class WindowAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = (dim // heads) ** -0.5\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, N, C)\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(\n",
        "            lambda t: t.view(B, N, self.heads, C // self.heads).transpose(1, 2),\n",
        "            qkv\n",
        "        )  # (B, heads, N, dim_head)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # (B, heads, N, N)\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = torch.matmul(attn, v)  # (B, heads, N, dim_head)\n",
        "        out = out.transpose(1, 2).reshape(B, N, C)  # (B, N, C)\n",
        "        out = self.to_out(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0641522",
      "metadata": {
        "id": "c0641522",
        "papermill": {
          "duration": 0.005776,
          "end_time": "2025-06-29T19:11:16.471172",
          "exception": false,
          "start_time": "2025-06-29T19:11:16.465396",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## MSFF_WinAttn_MobileNet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff3c7855",
      "metadata": {
        "id": "ff3c7855",
        "papermill": {
          "duration": 0.003281,
          "end_time": "2025-06-29T19:11:16.479847",
          "exception": false,
          "start_time": "2025-06-29T19:11:16.476566",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Multi-Scale Feature Fusion MobileNetV2 with Window Attention.\n",
        "\n",
        "  This model extracts multi-scale convolutional features from different\n",
        "  stages of MobileNetV2, reduces their channels to a uniform size, and\n",
        "  applies local windowed self-attention to model dependencies between scales."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18135c57",
      "metadata": {
        "id": "18135c57",
        "papermill": {
          "duration": 0.003197,
          "end_time": "2025-06-29T19:11:16.486533",
          "exception": false,
          "start_time": "2025-06-29T19:11:16.483336",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# MSFF_WinAttn_MobileNet\n",
        "\n",
        "## Overview\n",
        "`MSFF_WinAttn_MobileNet` is a hybrid deep learning model that combines:\n",
        "\n",
        "- **MobileNetV2 backbone** for efficient multi-scale feature extraction.\n",
        "- **1×1 convolutions** to normalize the channels of each feature stage.\n",
        "- **Windowed Multi-Head Self-Attention** to model dependencies across scales.\n",
        "- **Lightweight classifier** for final prediction.\n",
        "\n",
        "This design enables capturing both spatial and semantic information across multiple feature hierarchies while keeping the model computationally efficient.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Components\n",
        "\n",
        "### 1️> Multi-Scale Feature Extraction\n",
        "Features are extracted from 4 stages of MobileNetV2:\n",
        "- **Stage 1:** Early low-level features (channels: 24)\n",
        "- **Stage 2:** Mid-level features (channels: 32)\n",
        "- **Stage 3:** High-level features (channels: 96)\n",
        "- **Stage 4:** Final semantic features (channels: 1280)\n",
        "\n",
        "These stages provide rich, complementary information about the input image.\n",
        "\n",
        "---\n",
        "\n",
        "### 2️> Channel Reduction\n",
        "Each stage output is projected to **256 channels** using 1×1 convolutions:\n",
        "\n",
        "- `reduce1`: 24 → 256 channels\n",
        "- `reduce2`: 32 → 256 channels\n",
        "- `reduce3`: 96 → 256 channels\n",
        "- `reduce4`: 1280 → 256 channels\n",
        "\n",
        "This normalization simplifies subsequent attention and fusion.\n",
        "\n",
        "---\n",
        "\n",
        "### 3️> Window Attention\n",
        "After reduction and global average pooling, the 4 feature vectors are stacked to shape `(B, 4, 256)`.\n",
        "\n",
        "A **windowed attention mechanism** is applied:\n",
        "- `LayerNorm(256)`\n",
        "- `WindowAttention`: Multi-head self-attention (4 heads)\n",
        "- `LayerNorm(256)`\n",
        "\n",
        "This allows the model to learn relationships **between different scales**.\n",
        "\n",
        "---\n",
        "\n",
        "### 4️> Classifier\n",
        "The attended features are flattened and passed through a linear layer:\n",
        "- `nn.Linear(256 × 4, num_classes)`\n",
        "\n",
        "This produces the final logits for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c478ac0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-29T19:11:16.494545Z",
          "iopub.status.busy": "2025-06-29T19:11:16.494296Z",
          "iopub.status.idle": "2025-06-29T19:11:16.503375Z",
          "shell.execute_reply": "2025-06-29T19:11:16.502670Z"
        },
        "id": "7c478ac0",
        "papermill": {
          "duration": 0.014915,
          "end_time": "2025-06-29T19:11:16.504742",
          "exception": false,
          "start_time": "2025-06-29T19:11:16.489827",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class MSFF_WinAttn_MobileNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        mobilenet = models.mobilenet_v2(pretrained=True).features\n",
        "\n",
        "        # Feature stages\n",
        "        self.stage1 = mobilenet[:4]    # 24-d\n",
        "        self.stage2 = mobilenet[4:7]   # 32-d\n",
        "        self.stage3 = mobilenet[7:14]  # 96-d\n",
        "        self.stage4 = mobilenet[14:]   # 1280-d\n",
        "\n",
        "        # Reduce channels to 256 for fusion\n",
        "        self.reduce1 = nn.Conv2d(24, 256, 1)\n",
        "        self.reduce2 = nn.Conv2d(32, 256, 1)\n",
        "        self.reduce3 = nn.Conv2d(96, 256, 1)\n",
        "        self.reduce4 = nn.Conv2d(1280, 256, 1)\n",
        "\n",
        "        # Window attention with LayerNorm before and after\n",
        "        self.win_attn = nn.Sequential(\n",
        "            nn.LayerNorm(256),\n",
        "            WindowAttention(dim=256, heads=4),\n",
        "            nn.LayerNorm(256)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(256 * 4, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feature extraction\n",
        "        x1 = self.stage1(x)\n",
        "        x2 = self.stage2(x1)\n",
        "        x3 = self.stage3(x2)\n",
        "        x4 = self.stage4(x3)\n",
        "\n",
        "        # Reduce + GAP\n",
        "        x1 = F.adaptive_avg_pool2d(self.reduce1(x1), 1).flatten(1)\n",
        "        x2 = F.adaptive_avg_pool2d(self.reduce2(x2), 1).flatten(1)\n",
        "        x3 = F.adaptive_avg_pool2d(self.reduce3(x3), 1).flatten(1)\n",
        "        x4 = F.adaptive_avg_pool2d(self.reduce4(x4), 1).flatten(1)\n",
        "\n",
        "        # Stack multi-scale features\n",
        "        feats = torch.stack([x1, x2, x3, x4], dim=1)  # (B, 4, 256)\n",
        "\n",
        "        # Apply window attention\n",
        "        feats = self.win_attn(feats)\n",
        "\n",
        "        # Flatten and classify\n",
        "        out = feats.flatten(1)  # (B, 4*256)\n",
        "        return self.classifier(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf2e193d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-29T19:11:16.512726Z",
          "iopub.status.busy": "2025-06-29T19:11:16.512227Z",
          "iopub.status.idle": "2025-06-29T19:11:17.189875Z",
          "shell.execute_reply": "2025-06-29T19:11:17.188727Z"
        },
        "id": "cf2e193d",
        "papermill": {
          "duration": 0.683225,
          "end_time": "2025-06-29T19:11:17.191359",
          "exception": false,
          "start_time": "2025-06-29T19:11:16.508134",
          "status": "completed"
        },
        "tags": [],
        "outputId": "6d2d23dc-a001-4154-df86-d2e533f4904b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 109MB/s] \n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MSFF_WinAttn_MobileNet(num_classes=2).to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1e31672",
      "metadata": {
        "id": "b1e31672",
        "papermill": {
          "duration": 0.004205,
          "end_time": "2025-06-29T19:11:17.200054",
          "exception": false,
          "start_time": "2025-06-29T19:11:17.195849",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Train and Validation Rule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9df6f9de",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-29T19:11:17.209316Z",
          "iopub.status.busy": "2025-06-29T19:11:17.209011Z",
          "iopub.status.idle": "2025-06-29T19:11:17.220056Z",
          "shell.execute_reply": "2025-06-29T19:11:17.219484Z"
        },
        "id": "9df6f9de",
        "papermill": {
          "duration": 0.017025,
          "end_time": "2025-06-29T19:11:17.221257",
          "exception": false,
          "start_time": "2025-06-29T19:11:17.204232",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=50, save_dir='./models'):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_val_loss = float('inf')\n",
        "    best_acc_model_path = os.path.join(save_dir, 'best_model_val_acc.pth')\n",
        "    best_loss_model_path = os.path.join(save_dir, 'best_model_val_loss.pth')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc = correct / total\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_acc = val_correct / val_total\n",
        "        val_losses.append(avg_val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Save best accuracy model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), best_acc_model_path)\n",
        "            print(f\"Saved best accuracy model at epoch {epoch+1} with Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Save best loss model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), best_loss_model_path)\n",
        "            print(f\"Saved best loss model at epoch {epoch+1} with Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    print(f\"\\n Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "    print(f\" Best Validation Loss: {best_val_loss:.4f}\")\n",
        "    print(f\" Best Accuracy Model Saved at: {best_acc_model_path}\")\n",
        "    print(f\" Best Loss Model Saved at: {best_loss_model_path}\")\n",
        "\n",
        "    return train_losses, train_accuracies, val_losses, val_accuracies, best_acc_model_path, best_loss_model_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9b123be",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-29T19:11:17.229772Z",
          "iopub.status.busy": "2025-06-29T19:11:17.229544Z",
          "iopub.status.idle": "2025-06-29T20:33:20.361894Z",
          "shell.execute_reply": "2025-06-29T20:33:20.360908Z"
        },
        "id": "a9b123be",
        "papermill": {
          "duration": 4923.148164,
          "end_time": "2025-06-29T20:33:20.373311",
          "exception": false,
          "start_time": "2025-06-29T19:11:17.225147",
          "status": "completed"
        },
        "tags": [],
        "outputId": "0a44fbae-dfa5-432b-c69e-b316b9baeeaa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved best accuracy model at epoch 1 with Acc: 0.9076\n",
            "Saved best loss model at epoch 1 with Loss: 0.2200\n",
            "Epoch [1/150], Train Loss: 0.4279, Train Acc: 0.8453, Val Loss: 0.2200, Val Acc: 0.9076\n",
            "Epoch [2/150], Train Loss: 0.2624, Train Acc: 0.9003, Val Loss: 0.2419, Val Acc: 0.9076\n",
            "Saved best accuracy model at epoch 3 with Acc: 0.9265\n",
            "Saved best loss model at epoch 3 with Loss: 0.1849\n",
            "Epoch [3/150], Train Loss: 0.2533, Train Acc: 0.9019, Val Loss: 0.1849, Val Acc: 0.9265\n",
            "Epoch [4/150], Train Loss: 0.2328, Train Acc: 0.9112, Val Loss: 0.2274, Val Acc: 0.9123\n",
            "Saved best accuracy model at epoch 5 with Acc: 0.9479\n",
            "Saved best loss model at epoch 5 with Loss: 0.1732\n",
            "Epoch [5/150], Train Loss: 0.2140, Train Acc: 0.9138, Val Loss: 0.1732, Val Acc: 0.9479\n",
            "Epoch [6/150], Train Loss: 0.1952, Train Acc: 0.9200, Val Loss: 0.1783, Val Acc: 0.9336\n",
            "Epoch [7/150], Train Loss: 0.1816, Train Acc: 0.9283, Val Loss: 0.2087, Val Acc: 0.9289\n",
            "Epoch [8/150], Train Loss: 0.1843, Train Acc: 0.9252, Val Loss: 0.2134, Val Acc: 0.9313\n",
            "Epoch [9/150], Train Loss: 0.1777, Train Acc: 0.9278, Val Loss: 0.2732, Val Acc: 0.8981\n",
            "Epoch [10/150], Train Loss: 0.1642, Train Acc: 0.9367, Val Loss: 0.2001, Val Acc: 0.9313\n",
            "Epoch [11/150], Train Loss: 0.1378, Train Acc: 0.9434, Val Loss: 0.1909, Val Acc: 0.9431\n",
            "Epoch [12/150], Train Loss: 0.1376, Train Acc: 0.9465, Val Loss: 0.1936, Val Acc: 0.9360\n",
            "Epoch [13/150], Train Loss: 0.1373, Train Acc: 0.9491, Val Loss: 0.2125, Val Acc: 0.9265\n",
            "Epoch [14/150], Train Loss: 0.1267, Train Acc: 0.9517, Val Loss: 0.2009, Val Acc: 0.9289\n",
            "Epoch [15/150], Train Loss: 0.1360, Train Acc: 0.9481, Val Loss: 0.2116, Val Acc: 0.9218\n",
            "Epoch [16/150], Train Loss: 0.1357, Train Acc: 0.9481, Val Loss: 0.1995, Val Acc: 0.9289\n",
            "Epoch [17/150], Train Loss: 0.1258, Train Acc: 0.9517, Val Loss: 0.1995, Val Acc: 0.9289\n",
            "Epoch [18/150], Train Loss: 0.1300, Train Acc: 0.9481, Val Loss: 0.1927, Val Acc: 0.9360\n",
            "Epoch [19/150], Train Loss: 0.1378, Train Acc: 0.9465, Val Loss: 0.2129, Val Acc: 0.9218\n",
            "Epoch [20/150], Train Loss: 0.1399, Train Acc: 0.9481, Val Loss: 0.2131, Val Acc: 0.9218\n",
            "Epoch [21/150], Train Loss: 0.1281, Train Acc: 0.9517, Val Loss: 0.1967, Val Acc: 0.9336\n",
            "Epoch [22/150], Train Loss: 0.1434, Train Acc: 0.9439, Val Loss: 0.2028, Val Acc: 0.9313\n",
            "Epoch [23/150], Train Loss: 0.1334, Train Acc: 0.9450, Val Loss: 0.2034, Val Acc: 0.9265\n",
            "Epoch [24/150], Train Loss: 0.1464, Train Acc: 0.9486, Val Loss: 0.2127, Val Acc: 0.9242\n",
            "Epoch [25/150], Train Loss: 0.1325, Train Acc: 0.9486, Val Loss: 0.2141, Val Acc: 0.9265\n",
            "Epoch [26/150], Train Loss: 0.1444, Train Acc: 0.9418, Val Loss: 0.1960, Val Acc: 0.9336\n",
            "Epoch [27/150], Train Loss: 0.1292, Train Acc: 0.9486, Val Loss: 0.2165, Val Acc: 0.9242\n",
            "Epoch [28/150], Train Loss: 0.1374, Train Acc: 0.9491, Val Loss: 0.2076, Val Acc: 0.9242\n",
            "Epoch [29/150], Train Loss: 0.1431, Train Acc: 0.9455, Val Loss: 0.2047, Val Acc: 0.9289\n",
            "Epoch [30/150], Train Loss: 0.1304, Train Acc: 0.9476, Val Loss: 0.2020, Val Acc: 0.9289\n",
            "Epoch [31/150], Train Loss: 0.1468, Train Acc: 0.9486, Val Loss: 0.2090, Val Acc: 0.9265\n",
            "Epoch [32/150], Train Loss: 0.1402, Train Acc: 0.9418, Val Loss: 0.2008, Val Acc: 0.9289\n",
            "Epoch [33/150], Train Loss: 0.1309, Train Acc: 0.9481, Val Loss: 0.1967, Val Acc: 0.9336\n",
            "Epoch [34/150], Train Loss: 0.1475, Train Acc: 0.9481, Val Loss: 0.2131, Val Acc: 0.9242\n",
            "Epoch [35/150], Train Loss: 0.1287, Train Acc: 0.9528, Val Loss: 0.2267, Val Acc: 0.9218\n",
            "Epoch [36/150], Train Loss: 0.1393, Train Acc: 0.9439, Val Loss: 0.1994, Val Acc: 0.9313\n",
            "Epoch [37/150], Train Loss: 0.1587, Train Acc: 0.9429, Val Loss: 0.2104, Val Acc: 0.9218\n",
            "Epoch [38/150], Train Loss: 0.1208, Train Acc: 0.9533, Val Loss: 0.1993, Val Acc: 0.9289\n",
            "Epoch [39/150], Train Loss: 0.1297, Train Acc: 0.9486, Val Loss: 0.1965, Val Acc: 0.9360\n",
            "Epoch [40/150], Train Loss: 0.1412, Train Acc: 0.9413, Val Loss: 0.2033, Val Acc: 0.9265\n",
            "Epoch [41/150], Train Loss: 0.1467, Train Acc: 0.9413, Val Loss: 0.1981, Val Acc: 0.9313\n",
            "Epoch [42/150], Train Loss: 0.1189, Train Acc: 0.9481, Val Loss: 0.2063, Val Acc: 0.9265\n",
            "Epoch [43/150], Train Loss: 0.1485, Train Acc: 0.9434, Val Loss: 0.2078, Val Acc: 0.9265\n",
            "Epoch [44/150], Train Loss: 0.1259, Train Acc: 0.9465, Val Loss: 0.1992, Val Acc: 0.9313\n",
            "Epoch [45/150], Train Loss: 0.1420, Train Acc: 0.9444, Val Loss: 0.1981, Val Acc: 0.9336\n",
            "Epoch [46/150], Train Loss: 0.1251, Train Acc: 0.9569, Val Loss: 0.2026, Val Acc: 0.9265\n",
            "Epoch [47/150], Train Loss: 0.1362, Train Acc: 0.9439, Val Loss: 0.1978, Val Acc: 0.9336\n",
            "Epoch [48/150], Train Loss: 0.1343, Train Acc: 0.9538, Val Loss: 0.2016, Val Acc: 0.9289\n",
            "Epoch [49/150], Train Loss: 0.1459, Train Acc: 0.9455, Val Loss: 0.1974, Val Acc: 0.9336\n",
            "Epoch [50/150], Train Loss: 0.1282, Train Acc: 0.9470, Val Loss: 0.1909, Val Acc: 0.9336\n",
            "Epoch [51/150], Train Loss: 0.1394, Train Acc: 0.9429, Val Loss: 0.2111, Val Acc: 0.9265\n",
            "Epoch [52/150], Train Loss: 0.1382, Train Acc: 0.9486, Val Loss: 0.2094, Val Acc: 0.9265\n",
            "Epoch [53/150], Train Loss: 0.1286, Train Acc: 0.9502, Val Loss: 0.2066, Val Acc: 0.9265\n",
            "Epoch [54/150], Train Loss: 0.1259, Train Acc: 0.9502, Val Loss: 0.2121, Val Acc: 0.9242\n",
            "Epoch [55/150], Train Loss: 0.1344, Train Acc: 0.9455, Val Loss: 0.2056, Val Acc: 0.9265\n",
            "Epoch [56/150], Train Loss: 0.1455, Train Acc: 0.9408, Val Loss: 0.2038, Val Acc: 0.9289\n",
            "Epoch [57/150], Train Loss: 0.1357, Train Acc: 0.9413, Val Loss: 0.2019, Val Acc: 0.9289\n",
            "Epoch [58/150], Train Loss: 0.1348, Train Acc: 0.9439, Val Loss: 0.2232, Val Acc: 0.9218\n",
            "Epoch [59/150], Train Loss: 0.1392, Train Acc: 0.9470, Val Loss: 0.2068, Val Acc: 0.9265\n",
            "Epoch [60/150], Train Loss: 0.1448, Train Acc: 0.9398, Val Loss: 0.2037, Val Acc: 0.9289\n",
            "Epoch [61/150], Train Loss: 0.1202, Train Acc: 0.9553, Val Loss: 0.2047, Val Acc: 0.9265\n",
            "Epoch [62/150], Train Loss: 0.1351, Train Acc: 0.9476, Val Loss: 0.2022, Val Acc: 0.9289\n",
            "Epoch [63/150], Train Loss: 0.1332, Train Acc: 0.9465, Val Loss: 0.1964, Val Acc: 0.9336\n",
            "Epoch [64/150], Train Loss: 0.1292, Train Acc: 0.9512, Val Loss: 0.2026, Val Acc: 0.9289\n",
            "Epoch [65/150], Train Loss: 0.1317, Train Acc: 0.9481, Val Loss: 0.2071, Val Acc: 0.9265\n",
            "Epoch [66/150], Train Loss: 0.1384, Train Acc: 0.9424, Val Loss: 0.2081, Val Acc: 0.9289\n",
            "Epoch [67/150], Train Loss: 0.1367, Train Acc: 0.9413, Val Loss: 0.1940, Val Acc: 0.9336\n",
            "Epoch [68/150], Train Loss: 0.1418, Train Acc: 0.9465, Val Loss: 0.2054, Val Acc: 0.9265\n",
            "Epoch [69/150], Train Loss: 0.1292, Train Acc: 0.9476, Val Loss: 0.1938, Val Acc: 0.9313\n",
            "Epoch [70/150], Train Loss: 0.1187, Train Acc: 0.9528, Val Loss: 0.2004, Val Acc: 0.9289\n",
            "Epoch [71/150], Train Loss: 0.1342, Train Acc: 0.9491, Val Loss: 0.1961, Val Acc: 0.9313\n",
            "Epoch [72/150], Train Loss: 0.1331, Train Acc: 0.9491, Val Loss: 0.2049, Val Acc: 0.9265\n",
            "Epoch [73/150], Train Loss: 0.1490, Train Acc: 0.9460, Val Loss: 0.2047, Val Acc: 0.9289\n",
            "Epoch [74/150], Train Loss: 0.1405, Train Acc: 0.9403, Val Loss: 0.1940, Val Acc: 0.9336\n",
            "Epoch [75/150], Train Loss: 0.1386, Train Acc: 0.9470, Val Loss: 0.1920, Val Acc: 0.9360\n",
            "Epoch [76/150], Train Loss: 0.1397, Train Acc: 0.9444, Val Loss: 0.2031, Val Acc: 0.9289\n",
            "Epoch [77/150], Train Loss: 0.1379, Train Acc: 0.9413, Val Loss: 0.1959, Val Acc: 0.9336\n",
            "Epoch [78/150], Train Loss: 0.1147, Train Acc: 0.9548, Val Loss: 0.2036, Val Acc: 0.9265\n",
            "Epoch [79/150], Train Loss: 0.1309, Train Acc: 0.9403, Val Loss: 0.1933, Val Acc: 0.9336\n",
            "Epoch [80/150], Train Loss: 0.1343, Train Acc: 0.9481, Val Loss: 0.2048, Val Acc: 0.9265\n",
            "Epoch [81/150], Train Loss: 0.1312, Train Acc: 0.9450, Val Loss: 0.2085, Val Acc: 0.9265\n",
            "Epoch [82/150], Train Loss: 0.1212, Train Acc: 0.9522, Val Loss: 0.2021, Val Acc: 0.9289\n",
            "Epoch [83/150], Train Loss: 0.1276, Train Acc: 0.9502, Val Loss: 0.1960, Val Acc: 0.9336\n",
            "Epoch [84/150], Train Loss: 0.1358, Train Acc: 0.9444, Val Loss: 0.2037, Val Acc: 0.9289\n",
            "Epoch [85/150], Train Loss: 0.1519, Train Acc: 0.9465, Val Loss: 0.1979, Val Acc: 0.9289\n",
            "Epoch [86/150], Train Loss: 0.1484, Train Acc: 0.9408, Val Loss: 0.2107, Val Acc: 0.9242\n",
            "Epoch [87/150], Train Loss: 0.1376, Train Acc: 0.9470, Val Loss: 0.2120, Val Acc: 0.9265\n",
            "Epoch [88/150], Train Loss: 0.1495, Train Acc: 0.9398, Val Loss: 0.1986, Val Acc: 0.9336\n",
            "Epoch [89/150], Train Loss: 0.1356, Train Acc: 0.9496, Val Loss: 0.2052, Val Acc: 0.9289\n",
            "Epoch [90/150], Train Loss: 0.1351, Train Acc: 0.9476, Val Loss: 0.2062, Val Acc: 0.9265\n",
            "Epoch [91/150], Train Loss: 0.1380, Train Acc: 0.9486, Val Loss: 0.2118, Val Acc: 0.9265\n",
            "Epoch [92/150], Train Loss: 0.1429, Train Acc: 0.9450, Val Loss: 0.2045, Val Acc: 0.9289\n",
            "Epoch [93/150], Train Loss: 0.1308, Train Acc: 0.9470, Val Loss: 0.2039, Val Acc: 0.9289\n",
            "Epoch [94/150], Train Loss: 0.1336, Train Acc: 0.9465, Val Loss: 0.2054, Val Acc: 0.9265\n",
            "Epoch [95/150], Train Loss: 0.1363, Train Acc: 0.9429, Val Loss: 0.2004, Val Acc: 0.9289\n",
            "Epoch [96/150], Train Loss: 0.1401, Train Acc: 0.9476, Val Loss: 0.2055, Val Acc: 0.9265\n",
            "Epoch [97/150], Train Loss: 0.1262, Train Acc: 0.9517, Val Loss: 0.2073, Val Acc: 0.9265\n",
            "Epoch [98/150], Train Loss: 0.1258, Train Acc: 0.9522, Val Loss: 0.2054, Val Acc: 0.9265\n",
            "Epoch [99/150], Train Loss: 0.1444, Train Acc: 0.9413, Val Loss: 0.1983, Val Acc: 0.9313\n",
            "Epoch [100/150], Train Loss: 0.1419, Train Acc: 0.9387, Val Loss: 0.2001, Val Acc: 0.9289\n",
            "Epoch [101/150], Train Loss: 0.1416, Train Acc: 0.9413, Val Loss: 0.1980, Val Acc: 0.9289\n",
            "Epoch [102/150], Train Loss: 0.1421, Train Acc: 0.9418, Val Loss: 0.2011, Val Acc: 0.9265\n",
            "Epoch [103/150], Train Loss: 0.1323, Train Acc: 0.9481, Val Loss: 0.1972, Val Acc: 0.9313\n",
            "Epoch [104/150], Train Loss: 0.1377, Train Acc: 0.9460, Val Loss: 0.2089, Val Acc: 0.9265\n",
            "Epoch [105/150], Train Loss: 0.1399, Train Acc: 0.9418, Val Loss: 0.2199, Val Acc: 0.9242\n",
            "Epoch [106/150], Train Loss: 0.1315, Train Acc: 0.9455, Val Loss: 0.2088, Val Acc: 0.9242\n",
            "Epoch [107/150], Train Loss: 0.1318, Train Acc: 0.9439, Val Loss: 0.2096, Val Acc: 0.9289\n",
            "Epoch [108/150], Train Loss: 0.1348, Train Acc: 0.9486, Val Loss: 0.2077, Val Acc: 0.9289\n",
            "Epoch [109/150], Train Loss: 0.1467, Train Acc: 0.9470, Val Loss: 0.2214, Val Acc: 0.9265\n",
            "Epoch [110/150], Train Loss: 0.1262, Train Acc: 0.9486, Val Loss: 0.2046, Val Acc: 0.9289\n",
            "Epoch [111/150], Train Loss: 0.1213, Train Acc: 0.9564, Val Loss: 0.2045, Val Acc: 0.9289\n",
            "Epoch [112/150], Train Loss: 0.1534, Train Acc: 0.9382, Val Loss: 0.1956, Val Acc: 0.9336\n",
            "Epoch [113/150], Train Loss: 0.1400, Train Acc: 0.9465, Val Loss: 0.2032, Val Acc: 0.9289\n",
            "Epoch [114/150], Train Loss: 0.1394, Train Acc: 0.9429, Val Loss: 0.1967, Val Acc: 0.9336\n",
            "Epoch [115/150], Train Loss: 0.1281, Train Acc: 0.9481, Val Loss: 0.2043, Val Acc: 0.9265\n",
            "Epoch [116/150], Train Loss: 0.1640, Train Acc: 0.9460, Val Loss: 0.2324, Val Acc: 0.9218\n",
            "Epoch [117/150], Train Loss: 0.1242, Train Acc: 0.9574, Val Loss: 0.1975, Val Acc: 0.9336\n",
            "Epoch [118/150], Train Loss: 0.1238, Train Acc: 0.9481, Val Loss: 0.2003, Val Acc: 0.9289\n",
            "Epoch [119/150], Train Loss: 0.1227, Train Acc: 0.9574, Val Loss: 0.2062, Val Acc: 0.9242\n",
            "Epoch [120/150], Train Loss: 0.1308, Train Acc: 0.9476, Val Loss: 0.1955, Val Acc: 0.9313\n",
            "Epoch [121/150], Train Loss: 0.1251, Train Acc: 0.9517, Val Loss: 0.2019, Val Acc: 0.9265\n",
            "Epoch [122/150], Train Loss: 0.1255, Train Acc: 0.9507, Val Loss: 0.2117, Val Acc: 0.9265\n",
            "Epoch [123/150], Train Loss: 0.1451, Train Acc: 0.9377, Val Loss: 0.1982, Val Acc: 0.9289\n",
            "Epoch [124/150], Train Loss: 0.1228, Train Acc: 0.9543, Val Loss: 0.2066, Val Acc: 0.9265\n",
            "Epoch [125/150], Train Loss: 0.1350, Train Acc: 0.9455, Val Loss: 0.2057, Val Acc: 0.9265\n",
            "Epoch [126/150], Train Loss: 0.1270, Train Acc: 0.9476, Val Loss: 0.2000, Val Acc: 0.9313\n",
            "Epoch [127/150], Train Loss: 0.1447, Train Acc: 0.9434, Val Loss: 0.1962, Val Acc: 0.9336\n",
            "Epoch [128/150], Train Loss: 0.1304, Train Acc: 0.9476, Val Loss: 0.1999, Val Acc: 0.9289\n",
            "Epoch [129/150], Train Loss: 0.1389, Train Acc: 0.9418, Val Loss: 0.2077, Val Acc: 0.9265\n",
            "Epoch [130/150], Train Loss: 0.1513, Train Acc: 0.9346, Val Loss: 0.2012, Val Acc: 0.9289\n",
            "Epoch [131/150], Train Loss: 0.1386, Train Acc: 0.9424, Val Loss: 0.2031, Val Acc: 0.9289\n",
            "Epoch [132/150], Train Loss: 0.1231, Train Acc: 0.9502, Val Loss: 0.2036, Val Acc: 0.9265\n",
            "Epoch [133/150], Train Loss: 0.1234, Train Acc: 0.9481, Val Loss: 0.2067, Val Acc: 0.9289\n",
            "Epoch [134/150], Train Loss: 0.1327, Train Acc: 0.9481, Val Loss: 0.1988, Val Acc: 0.9313\n",
            "Epoch [135/150], Train Loss: 0.1242, Train Acc: 0.9512, Val Loss: 0.1950, Val Acc: 0.9336\n",
            "Epoch [136/150], Train Loss: 0.1350, Train Acc: 0.9496, Val Loss: 0.1992, Val Acc: 0.9289\n",
            "Epoch [137/150], Train Loss: 0.1242, Train Acc: 0.9496, Val Loss: 0.2018, Val Acc: 0.9289\n",
            "Epoch [138/150], Train Loss: 0.1269, Train Acc: 0.9491, Val Loss: 0.2042, Val Acc: 0.9265\n",
            "Epoch [139/150], Train Loss: 0.1486, Train Acc: 0.9424, Val Loss: 0.2078, Val Acc: 0.9242\n",
            "Epoch [140/150], Train Loss: 0.1361, Train Acc: 0.9465, Val Loss: 0.1964, Val Acc: 0.9313\n",
            "Epoch [141/150], Train Loss: 0.1259, Train Acc: 0.9507, Val Loss: 0.2013, Val Acc: 0.9289\n",
            "Epoch [142/150], Train Loss: 0.1401, Train Acc: 0.9403, Val Loss: 0.1900, Val Acc: 0.9384\n",
            "Epoch [143/150], Train Loss: 0.1332, Train Acc: 0.9502, Val Loss: 0.2017, Val Acc: 0.9289\n",
            "Epoch [144/150], Train Loss: 0.1317, Train Acc: 0.9403, Val Loss: 0.2031, Val Acc: 0.9289\n",
            "Epoch [145/150], Train Loss: 0.1403, Train Acc: 0.9434, Val Loss: 0.2032, Val Acc: 0.9289\n",
            "Epoch [146/150], Train Loss: 0.1336, Train Acc: 0.9444, Val Loss: 0.2017, Val Acc: 0.9289\n",
            "Epoch [147/150], Train Loss: 0.1335, Train Acc: 0.9543, Val Loss: 0.2045, Val Acc: 0.9265\n",
            "Epoch [148/150], Train Loss: 0.1447, Train Acc: 0.9444, Val Loss: 0.2025, Val Acc: 0.9289\n",
            "Epoch [149/150], Train Loss: 0.1374, Train Acc: 0.9418, Val Loss: 0.2075, Val Acc: 0.9265\n",
            "Epoch [150/150], Train Loss: 0.1350, Train Acc: 0.9434, Val Loss: 0.2235, Val Acc: 0.9242\n",
            "\n",
            " Best Validation Accuracy: 0.9479\n",
            " Best Validation Loss: 0.1732\n",
            " Best Accuracy Model Saved at: ./models/best_model_val_acc.pth\n",
            " Best Loss Model Saved at: ./models/best_model_val_loss.pth\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "([0.4278855932052018,\n",
              "  0.2623553396981271,\n",
              "  0.2532828080116725,\n",
              "  0.23279771733967985,\n",
              "  0.21400359639378844,\n",
              "  0.19522300106091578,\n",
              "  0.18158807562755758,\n",
              "  0.18428710066392773,\n",
              "  0.17766953116191214,\n",
              "  0.16422649051566593,\n",
              "  0.13776733023954219,\n",
              "  0.13760521391131839,\n",
              "  0.1373230644089521,\n",
              "  0.12673743307346202,\n",
              "  0.13599954606568226,\n",
              "  0.13571694264280015,\n",
              "  0.12581864665033388,\n",
              "  0.1300470737099159,\n",
              "  0.1377530941342721,\n",
              "  0.13994664020958494,\n",
              "  0.1281251076181404,\n",
              "  0.14338458888232708,\n",
              "  0.13336603071723804,\n",
              "  0.1463704417597075,\n",
              "  0.13251168983148748,\n",
              "  0.14438553978918028,\n",
              "  0.12924033788139702,\n",
              "  0.13736821014861592,\n",
              "  0.14307107871062444,\n",
              "  0.13039711233778079,\n",
              "  0.14681664958107668,\n",
              "  0.14019033911286807,\n",
              "  0.13087697913412188,\n",
              "  0.14754252551031893,\n",
              "  0.12870498829078478,\n",
              "  0.13934270356644374,\n",
              "  0.15873195539366025,\n",
              "  0.1208258787261658,\n",
              "  0.12968715075708803,\n",
              "  0.14121067592660425,\n",
              "  0.14667488069685755,\n",
              "  0.11889416641998486,\n",
              "  0.14849366656825191,\n",
              "  0.12589693020601742,\n",
              "  0.14195039840873147,\n",
              "  0.12513385484086686,\n",
              "  0.13620086649402244,\n",
              "  0.13433556838846597,\n",
              "  0.1458528504752722,\n",
              "  0.1281944157891586,\n",
              "  0.13943006782258144,\n",
              "  0.1381652642835359,\n",
              "  0.12858046517997493,\n",
              "  0.1259430763540698,\n",
              "  0.1344093642976196,\n",
              "  0.14546971087206584,\n",
              "  0.13569657104547883,\n",
              "  0.1347653892074452,\n",
              "  0.13916470745547874,\n",
              "  0.14477649905153964,\n",
              "  0.12023892018516533,\n",
              "  0.13510045174081795,\n",
              "  0.13323513195529335,\n",
              "  0.12915007220428498,\n",
              "  0.1316657049856225,\n",
              "  0.13839879042667444,\n",
              "  0.13673731910644984,\n",
              "  0.14177667606072347,\n",
              "  0.12916618518409181,\n",
              "  0.11868402001554848,\n",
              "  0.13421369200480765,\n",
              "  0.13308060031811722,\n",
              "  0.14897564151247994,\n",
              "  0.14046069155218172,\n",
              "  0.1385722941734263,\n",
              "  0.1397265938461804,\n",
              "  0.13790625776545923,\n",
              "  0.11471744009950122,\n",
              "  0.1308839333655893,\n",
              "  0.13432392761966244,\n",
              "  0.1311852610746368,\n",
              "  0.12123806857061191,\n",
              "  0.12759499392304263,\n",
              "  0.13579163056050167,\n",
              "  0.15194547090862617,\n",
              "  0.14837816461432177,\n",
              "  0.13755386396021138,\n",
              "  0.14945320540764293,\n",
              "  0.13555246399196444,\n",
              "  0.13505606435727877,\n",
              "  0.13799361985360012,\n",
              "  0.1429204209173312,\n",
              "  0.1308479739017174,\n",
              "  0.1335678349752895,\n",
              "  0.13625373642464153,\n",
              "  0.14008255125802072,\n",
              "  0.12618128075951435,\n",
              "  0.1257882357010099,\n",
              "  0.1444336384779117,\n",
              "  0.14188675435840106,\n",
              "  0.14155199450486508,\n",
              "  0.14205447169112378,\n",
              "  0.1322739250102981,\n",
              "  0.13767950233743817,\n",
              "  0.13993407914140185,\n",
              "  0.13146559197883137,\n",
              "  0.13182774763248983,\n",
              "  0.1347524680075098,\n",
              "  0.14665688439959385,\n",
              "  0.12621520853555593,\n",
              "  0.12134866663789162,\n",
              "  0.1534356305711582,\n",
              "  0.14001319963546072,\n",
              "  0.13939088137179126,\n",
              "  0.12814939761015234,\n",
              "  0.16401423422283815,\n",
              "  0.12417880864050544,\n",
              "  0.123771853989265,\n",
              "  0.1227296702258411,\n",
              "  0.1307506072716635,\n",
              "  0.12507331884298167,\n",
              "  0.12551660719709318,\n",
              "  0.14511891332317572,\n",
              "  0.1227733067191038,\n",
              "  0.13504584774863523,\n",
              "  0.12700338350211987,\n",
              "  0.14473574889487908,\n",
              "  0.13038076854265127,\n",
              "  0.13894297249737333,\n",
              "  0.1512764921625618,\n",
              "  0.13864434492148336,\n",
              "  0.12308432925187174,\n",
              "  0.12341119400912622,\n",
              "  0.132743328962414,\n",
              "  0.12419773682524435,\n",
              "  0.1349537092276284,\n",
              "  0.12421119591740311,\n",
              "  0.12689565403051065,\n",
              "  0.14860051632171772,\n",
              "  0.1360666079171857,\n",
              "  0.12588613419259181,\n",
              "  0.14011018802640868,\n",
              "  0.13322352495838385,\n",
              "  0.1316667952742733,\n",
              "  0.14032086235333663,\n",
              "  0.1336130612209195,\n",
              "  0.13350226421703082,\n",
              "  0.1447495513397162,\n",
              "  0.13739825901193697,\n",
              "  0.13496854586801568],\n",
              " [0.8452751817237798,\n",
              "  0.9003115264797508,\n",
              "  0.9018691588785047,\n",
              "  0.9112149532710281,\n",
              "  0.9138110072689511,\n",
              "  0.9200415368639667,\n",
              "  0.9283489096573209,\n",
              "  0.9252336448598131,\n",
              "  0.9278296988577363,\n",
              "  0.936656282450675,\n",
              "  0.9434060228452752,\n",
              "  0.946521287642783,\n",
              "  0.9491173416407062,\n",
              "  0.9517133956386293,\n",
              "  0.9480789200415368,\n",
              "  0.9480789200415368,\n",
              "  0.9517133956386293,\n",
              "  0.9480789200415368,\n",
              "  0.946521287642783,\n",
              "  0.9480789200415368,\n",
              "  0.9517133956386293,\n",
              "  0.9439252336448598,\n",
              "  0.944963655244029,\n",
              "  0.9485981308411215,\n",
              "  0.9485981308411215,\n",
              "  0.9418483904465212,\n",
              "  0.9485981308411215,\n",
              "  0.9491173416407062,\n",
              "  0.9454828660436138,\n",
              "  0.9475597092419522,\n",
              "  0.9485981308411215,\n",
              "  0.9418483904465212,\n",
              "  0.9480789200415368,\n",
              "  0.9480789200415368,\n",
              "  0.9527518172377986,\n",
              "  0.9439252336448598,\n",
              "  0.9428868120456906,\n",
              "  0.9532710280373832,\n",
              "  0.9485981308411215,\n",
              "  0.9413291796469366,\n",
              "  0.9413291796469366,\n",
              "  0.9480789200415368,\n",
              "  0.9434060228452752,\n",
              "  0.946521287642783,\n",
              "  0.9444444444444444,\n",
              "  0.9569055036344756,\n",
              "  0.9439252336448598,\n",
              "  0.9537902388369678,\n",
              "  0.9454828660436138,\n",
              "  0.9470404984423676,\n",
              "  0.9428868120456906,\n",
              "  0.9485981308411215,\n",
              "  0.9501557632398754,\n",
              "  0.9501557632398754,\n",
              "  0.9454828660436138,\n",
              "  0.940809968847352,\n",
              "  0.9413291796469366,\n",
              "  0.9439252336448598,\n",
              "  0.9470404984423676,\n",
              "  0.9397715472481828,\n",
              "  0.9553478712357217,\n",
              "  0.9475597092419522,\n",
              "  0.946521287642783,\n",
              "  0.9511941848390446,\n",
              "  0.9480789200415368,\n",
              "  0.942367601246106,\n",
              "  0.9413291796469366,\n",
              "  0.946521287642783,\n",
              "  0.9475597092419522,\n",
              "  0.9527518172377986,\n",
              "  0.9491173416407062,\n",
              "  0.9491173416407062,\n",
              "  0.9460020768431984,\n",
              "  0.9402907580477674,\n",
              "  0.9470404984423676,\n",
              "  0.9444444444444444,\n",
              "  0.9413291796469366,\n",
              "  0.9548286604361371,\n",
              "  0.9402907580477674,\n",
              "  0.9480789200415368,\n",
              "  0.944963655244029,\n",
              "  0.952232606438214,\n",
              "  0.9501557632398754,\n",
              "  0.9444444444444444,\n",
              "  0.946521287642783,\n",
              "  0.940809968847352,\n",
              "  0.9470404984423676,\n",
              "  0.9397715472481828,\n",
              "  0.9496365524402908,\n",
              "  0.9475597092419522,\n",
              "  0.9485981308411215,\n",
              "  0.944963655244029,\n",
              "  0.9470404984423676,\n",
              "  0.946521287642783,\n",
              "  0.9428868120456906,\n",
              "  0.9475597092419522,\n",
              "  0.9517133956386293,\n",
              "  0.952232606438214,\n",
              "  0.9413291796469366,\n",
              "  0.9387331256490135,\n",
              "  0.9413291796469366,\n",
              "  0.9418483904465212,\n",
              "  0.9480789200415368,\n",
              "  0.9460020768431984,\n",
              "  0.9418483904465212,\n",
              "  0.9454828660436138,\n",
              "  0.9439252336448598,\n",
              "  0.9485981308411215,\n",
              "  0.9470404984423676,\n",
              "  0.9485981308411215,\n",
              "  0.956386292834891,\n",
              "  0.9382139148494288,\n",
              "  0.946521287642783,\n",
              "  0.9428868120456906,\n",
              "  0.9480789200415368,\n",
              "  0.9460020768431984,\n",
              "  0.9574247144340602,\n",
              "  0.9480789200415368,\n",
              "  0.9574247144340602,\n",
              "  0.9475597092419522,\n",
              "  0.9517133956386293,\n",
              "  0.95067497403946,\n",
              "  0.9376947040498442,\n",
              "  0.9543094496365524,\n",
              "  0.9454828660436138,\n",
              "  0.9475597092419522,\n",
              "  0.9434060228452752,\n",
              "  0.9475597092419522,\n",
              "  0.9418483904465212,\n",
              "  0.9345794392523364,\n",
              "  0.942367601246106,\n",
              "  0.9501557632398754,\n",
              "  0.9480789200415368,\n",
              "  0.9480789200415368,\n",
              "  0.9511941848390446,\n",
              "  0.9496365524402908,\n",
              "  0.9496365524402908,\n",
              "  0.9491173416407062,\n",
              "  0.942367601246106,\n",
              "  0.946521287642783,\n",
              "  0.95067497403946,\n",
              "  0.9402907580477674,\n",
              "  0.9501557632398754,\n",
              "  0.9402907580477674,\n",
              "  0.9434060228452752,\n",
              "  0.9444444444444444,\n",
              "  0.9543094496365524,\n",
              "  0.9444444444444444,\n",
              "  0.9418483904465212,\n",
              "  0.9434060228452752],\n",
              " [0.21995684943561042,\n",
              "  0.24185786609138762,\n",
              "  0.18491214853046195,\n",
              "  0.22736548782891727,\n",
              "  0.17322130032282854,\n",
              "  0.1783038493595086,\n",
              "  0.2086646501307509,\n",
              "  0.21340135312805483,\n",
              "  0.2731680330886905,\n",
              "  0.2001362370543315,\n",
              "  0.19086158234027348,\n",
              "  0.19362762952057114,\n",
              "  0.21253240123041905,\n",
              "  0.20093333317033416,\n",
              "  0.2116390379461726,\n",
              "  0.19945023868266226,\n",
              "  0.19947951567259484,\n",
              "  0.19272872241812625,\n",
              "  0.21288696841137217,\n",
              "  0.21307191810254672,\n",
              "  0.1966869010024571,\n",
              "  0.20281381843545074,\n",
              "  0.2033530338251564,\n",
              "  0.2127105223522189,\n",
              "  0.21413583319268323,\n",
              "  0.19599924216579115,\n",
              "  0.21653205503493414,\n",
              "  0.2075691803641218,\n",
              "  0.20470831715335538,\n",
              "  0.2019671923480928,\n",
              "  0.20901511607059678,\n",
              "  0.2007571319201296,\n",
              "  0.19669866726534174,\n",
              "  0.21310754802211054,\n",
              "  0.22666896665551967,\n",
              "  0.1994471494773669,\n",
              "  0.21043401699612982,\n",
              "  0.1993224732078878,\n",
              "  0.19650319780755257,\n",
              "  0.20334988590496192,\n",
              "  0.19808912036075657,\n",
              "  0.20627347142934532,\n",
              "  0.20783874102302693,\n",
              "  0.19922128549244786,\n",
              "  0.19813428854104131,\n",
              "  0.20261898349937318,\n",
              "  0.19777671381598338,\n",
              "  0.20157412322337873,\n",
              "  0.19738229130494542,\n",
              "  0.19086069831558103,\n",
              "  0.2111473023126434,\n",
              "  0.20941467528297966,\n",
              "  0.20663170003431983,\n",
              "  0.21208717742618838,\n",
              "  0.20555603780251527,\n",
              "  0.20383586117532104,\n",
              "  0.20194260183156335,\n",
              "  0.22322975339401246,\n",
              "  0.20684913257303247,\n",
              "  0.20368815713196195,\n",
              "  0.20469262685427175,\n",
              "  0.2021587980811351,\n",
              "  0.19638424501421728,\n",
              "  0.20258471868666156,\n",
              "  0.20712261148894737,\n",
              "  0.208133293332399,\n",
              "  0.19401897755285194,\n",
              "  0.20535239938180894,\n",
              "  0.1938441081271906,\n",
              "  0.20041406520509294,\n",
              "  0.19608171239295707,\n",
              "  0.2049037872597442,\n",
              "  0.20468205166149087,\n",
              "  0.1940191272629558,\n",
              "  0.19201247990297685,\n",
              "  0.20307779464305245,\n",
              "  0.19593567673083662,\n",
              "  0.20356723364342802,\n",
              "  0.19328588164145394,\n",
              "  0.20475391211220995,\n",
              "  0.20845707139233127,\n",
              "  0.20213519541513442,\n",
              "  0.19599407487216272,\n",
              "  0.20374483867116006,\n",
              "  0.1979181564363119,\n",
              "  0.21072821941925213,\n",
              "  0.21201975308524976,\n",
              "  0.19858975176300322,\n",
              "  0.20518167726030306,\n",
              "  0.2062231703062675,\n",
              "  0.21184830238676763,\n",
              "  0.20452970169052215,\n",
              "  0.20392558036837727,\n",
              "  0.20542120718995907,\n",
              "  0.20044662625462348,\n",
              "  0.20553398167248815,\n",
              "  0.2073440688878431,\n",
              "  0.20541118560192576,\n",
              "  0.1983145982293146,\n",
              "  0.2001486941589974,\n",
              "  0.19803751617603535,\n",
              "  0.20110877335537225,\n",
              "  0.19717755217737118,\n",
              "  0.20891262117740034,\n",
              "  0.2199097255527574,\n",
              "  0.2087993515139845,\n",
              "  0.20961327651249512,\n",
              "  0.20771537180657365,\n",
              "  0.2213731017545797,\n",
              "  0.20462731283623725,\n",
              "  0.20451796198696165,\n",
              "  0.1956409351135205,\n",
              "  0.20323872929605255,\n",
              "  0.19667509825168444,\n",
              "  0.20434711390407756,\n",
              "  0.2323716279684699,\n",
              "  0.19745463305818184,\n",
              "  0.2003497180246216,\n",
              "  0.2062402791426783,\n",
              "  0.19554552739386313,\n",
              "  0.20194502836758538,\n",
              "  0.21167468617204577,\n",
              "  0.19816532931456873,\n",
              "  0.2066471924356717,\n",
              "  0.20570837112609297,\n",
              "  0.19995587110419624,\n",
              "  0.19615133709573587,\n",
              "  0.1999420364071349,\n",
              "  0.20765100270676026,\n",
              "  0.20119116674011042,\n",
              "  0.20306556834839284,\n",
              "  0.20358709546936943,\n",
              "  0.20671660072236722,\n",
              "  0.19879325062668482,\n",
              "  0.19503424497919955,\n",
              "  0.19924074344869172,\n",
              "  0.201787334850191,\n",
              "  0.204229500427443,\n",
              "  0.20780940285684274,\n",
              "  0.19640987523598596,\n",
              "  0.2012951196187974,\n",
              "  0.19001456861483998,\n",
              "  0.20171328041137063,\n",
              "  0.20307447289815173,\n",
              "  0.20316376362461597,\n",
              "  0.20174014301405155,\n",
              "  0.20450154630400771,\n",
              "  0.20247633268757323,\n",
              "  0.20747207293918887,\n",
              "  0.22351270271298876],\n",
              " [0.9075829383886256,\n",
              "  0.9075829383886256,\n",
              "  0.9265402843601895,\n",
              "  0.9123222748815166,\n",
              "  0.9478672985781991,\n",
              "  0.933649289099526,\n",
              "  0.9289099526066351,\n",
              "  0.9312796208530806,\n",
              "  0.8981042654028436,\n",
              "  0.9312796208530806,\n",
              "  0.943127962085308,\n",
              "  0.9360189573459715,\n",
              "  0.9265402843601895,\n",
              "  0.9289099526066351,\n",
              "  0.9218009478672986,\n",
              "  0.9289099526066351,\n",
              "  0.9289099526066351,\n",
              "  0.9360189573459715,\n",
              "  0.9218009478672986,\n",
              "  0.9218009478672986,\n",
              "  0.933649289099526,\n",
              "  0.9312796208530806,\n",
              "  0.9265402843601895,\n",
              "  0.9241706161137441,\n",
              "  0.9265402843601895,\n",
              "  0.933649289099526,\n",
              "  0.9241706161137441,\n",
              "  0.9241706161137441,\n",
              "  0.9289099526066351,\n",
              "  0.9289099526066351,\n",
              "  0.9265402843601895,\n",
              "  0.9289099526066351,\n",
              "  0.933649289099526,\n",
              "  0.9241706161137441,\n",
              "  0.9218009478672986,\n",
              "  0.9312796208530806,\n",
              "  0.9218009478672986,\n",
              "  0.9289099526066351,\n",
              "  0.9360189573459715,\n",
              "  0.9265402843601895,\n",
              "  0.9312796208530806,\n",
              "  0.9265402843601895,\n",
              "  0.9265402843601895,\n",
              "  0.9312796208530806,\n",
              "  0.933649289099526,\n",
              "  0.9265402843601895,\n",
              "  0.933649289099526,\n",
              "  0.9289099526066351,\n",
              "  0.933649289099526,\n",
              "  0.933649289099526,\n",
              "  0.9265402843601895,\n",
              "  0.9265402843601895,\n",
              "  0.9265402843601895,\n",
              "  0.9241706161137441,\n",
              "  0.9265402843601895,\n",
              "  0.9289099526066351,\n",
              "  0.9289099526066351,\n",
              "  0.9218009478672986,\n",
              "  0.9265402843601895,\n",
              "  0.9289099526066351,\n",
              "  0.9265402843601895,\n",
              "  0.9289099526066351,\n",
              "  0.933649289099526,\n",
              "  0.9289099526066351,\n",
              "  0.9265402843601895,\n",
              "  0.9289099526066351,\n",
              "  0.933649289099526,\n",
              "  0.9265402843601895,\n",
              "  0.9312796208530806,\n",
              "  0.9289099526066351,\n",
              "  0.9312796208530806,\n",
              "  0.9265402843601895,\n",
              "  0.9289099526066351,\n",
              "  0.933649289099526,\n",
              "  0.9360189573459715,\n",
              "  0.9289099526066351,\n",
              "  0.933649289099526,\n",
              "  0.9265402843601895,\n",
              "  0.933649289099526,\n",
              "  0.9265402843601895,\n",
              "  0.9265402843601895,\n",
              "  0.9289099526066351,\n",
              "  0.933649289099526,\n",
              "  0.9289099526066351,\n",
              "  0.9289099526066351,\n",
              "  0.9241706161137441,\n",
              "  0.9265402843601895,\n",
              "  0.933649289099526,\n",
              "  0.9289099526066351,\n",
              "  0.9265402843601895,\n",
              "  0.9265402843601895,\n",
              "  0.9289099526066351,\n",
              "  0.9289099526066351,\n",
              "  0.9265402843601895,\n",
              "  0.9289099526066351,\n",
              "  0.9265402843601895,\n",
              "  0.9265402843601895,\n",
              "  0.9265402843601895,\n",
              "  0.9312796208530806,\n",
              "  0.9289099526066351,\n",
              "  0.9289099526066351,\n",
              "  0.9265402843601895,\n",
              "  0.9312796208530806,\n",
              "  0.9265402843601895,\n",
              "  0.9241706161137441,\n",
              "  0.9241706161137441,\n",
              "  0.9289099526066351,\n",
              "  0.9289099526066351,\n",
              "  0.9265402843601895,\n",
              "  0.9289099526066351,\n",
              "  0.9289099526066351,\n",
              "  0.933649289099526,\n",
              "  0.9289099526066351,\n",
              "  0.933649289099526,\n",
              "  0.9265402843601895,\n",
              "  0.9218009478672986,\n",
              "  0.933649289099526,\n",
              "  0.9289099526066351,\n",
              "  0.9241706161137441,\n",
              "  0.9312796208530806,\n",
              "  0.9265402843601895,\n",
              "  0.9265402843601895,\n",
              "  0.9289099526066351,\n",
              "  0.9265402843601895,\n",
              "  0.9265402843601895,\n",
              "  0.9312796208530806,\n",
              "  0.933649289099526,\n",
              "  0.9289099526066351,\n",
              "  0.9265402843601895,\n",
              "  0.9289099526066351,\n",
              "  0.9289099526066351,\n",
              "  0.9265402843601895,\n",
              "  0.9289099526066351,\n",
              "  0.9312796208530806,\n",
              "  0.933649289099526,\n",
              "  0.9289099526066351,\n",
              "  0.9289099526066351,\n",
              "  0.9265402843601895,\n",
              "  0.9241706161137441,\n",
              "  0.9312796208530806,\n",
              "  0.9289099526066351,\n",
              "  0.9383886255924171,\n",
              "  0.9289099526066351,\n",
              "  0.9289099526066351,\n",
              "  0.9289099526066351,\n",
              "  0.9289099526066351,\n",
              "  0.9265402843601895,\n",
              "  0.9289099526066351,\n",
              "  0.9265402843601895,\n",
              "  0.9241706161137441],\n",
              " './models/best_model_val_acc.pth',\n",
              " './models/best_model_val_loss.pth')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a1f4a3e",
      "metadata": {
        "id": "1a1f4a3e",
        "papermill": {
          "duration": 0.011214,
          "end_time": "2025-06-29T20:33:20.396165",
          "exception": false,
          "start_time": "2025-06-29T20:33:20.384951",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Test (Code for Test dataset with folder path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71ae6d2c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-29T20:33:20.420398Z",
          "iopub.status.busy": "2025-06-29T20:33:20.420058Z",
          "iopub.status.idle": "2025-06-29T20:33:20.431376Z",
          "shell.execute_reply": "2025-06-29T20:33:20.430718Z"
        },
        "id": "71ae6d2c",
        "papermill": {
          "duration": 0.025245,
          "end_time": "2025-06-29T20:33:20.432772",
          "exception": false,
          "start_time": "2025-06-29T20:33:20.407527",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_albumentations_test_transform():\n",
        "    return A.Compose([\n",
        "        A.Resize(224, 224),\n",
        "        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "class AlbumentationsDataset(ImageFolder):\n",
        "    def __init__(self, root, transform=None):\n",
        "        super().__init__(root)\n",
        "        self.albumentations_transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, label = super().__getitem__(index)\n",
        "        image = np.array(image)\n",
        "        if self.albumentations_transform:\n",
        "            image = self.albumentations_transform(image=image)['image']\n",
        "        return image, label\n",
        "\n",
        "def test_model(model, model_path, test_folder, device='cuda' if torch.cuda.is_available() else 'cpu', batch_size=32):\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_transform = get_albumentations_test_transform()\n",
        "    test_dataset = AlbumentationsDataset(test_folder, transform=test_transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    rec = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"\\n Test Accuracy: {acc:.4f}\")\n",
        "    print(f\" Precision:     {prec:.4f}\")\n",
        "    print(f\" Recall:        {rec:.4f}\")\n",
        "    print(f\" F1 Score:      {f1:.4f}\")\n",
        "\n",
        "    return acc, prec, rec, f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f51a4aee",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-29T20:33:20.456979Z",
          "iopub.status.busy": "2025-06-29T20:33:20.456302Z",
          "iopub.status.idle": "2025-06-29T20:33:24.784346Z",
          "shell.execute_reply": "2025-06-29T20:33:24.783480Z"
        },
        "id": "f51a4aee",
        "papermill": {
          "duration": 4.341053,
          "end_time": "2025-06-29T20:33:24.785677",
          "exception": false,
          "start_time": "2025-06-29T20:33:20.444624",
          "status": "completed"
        },
        "tags": [],
        "outputId": "222cf15d-ecb5-4706-a635-91081db98f66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Test Accuracy: 0.9479\n",
            " Precision:     0.9293\n",
            " Recall:        0.8949\n",
            " F1 Score:      0.9108\n"
          ]
        }
      ],
      "source": [
        "test_folder = \"/kaggle/input/comys-hackathon5-2025/Comys_Hackathon5/Task_A/val\"\n",
        "model_path = \"/kaggle/working/models/best_model_val_acc.pth\"\n",
        "\n",
        "acc, prec, rec, f1 = test_model(model, model_path, test_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_folder = \"/content/extracted_folder/Comys_Hackathon5/Task_A/train\"\n",
        "model_path = \"/content/best_model_val_loss.pth\"\n",
        "\n",
        "acc, prec, rec, f1 = test_model(model, model_path, test_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiUqUyS5uj10",
        "outputId": "c8fc8c23-718b-41f0-e6c4-540538e4dc33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Test Accuracy: 0.9709\n",
            " Precision:     0.9383\n",
            " Recall:        0.9546\n",
            " F1 Score:      0.9462\n"
          ]
        }
      ],
      "id": "GiUqUyS5uj10"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "APGFCZD0Fncg",
        "l6bToUx6GD4P",
        "mjfTRgYwLrWD",
        "leVyI6jMJmE5"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 7766818,
          "sourceId": 12321727,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 4958.530846,
      "end_time": "2025-06-29T20:33:32.299198",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-06-29T19:10:53.768352",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}