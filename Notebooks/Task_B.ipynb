{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5928cc0c",
      "metadata": {
        "id": "5928cc0c",
        "papermill": {
          "duration": 0.003892,
          "end_time": "2025-06-30T03:01:14.979878",
          "exception": false,
          "start_time": "2025-06-30T03:01:14.975986",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7836d3ae",
      "metadata": {
        "id": "7836d3ae",
        "papermill": {
          "duration": 0.002883,
          "end_time": "2025-06-30T03:01:14.985971",
          "exception": false,
          "start_time": "2025-06-30T03:01:14.983088",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d7d2d85",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-30T03:01:14.992889Z",
          "iopub.status.busy": "2025-06-30T03:01:14.992660Z",
          "iopub.status.idle": "2025-06-30T03:01:27.899700Z",
          "shell.execute_reply": "2025-06-30T03:01:27.898816Z"
        },
        "id": "0d7d2d85",
        "papermill": {
          "duration": 12.912325,
          "end_time": "2025-06-30T03:01:27.901308",
          "exception": false,
          "start_time": "2025-06-30T03:01:14.988983",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torchvision import transforms\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb8a0b09",
      "metadata": {
        "id": "eb8a0b09",
        "papermill": {
          "duration": 0.003555,
          "end_time": "2025-06-30T03:01:27.908866",
          "exception": false,
          "start_time": "2025-06-30T03:01:27.905311",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Balanced Dataset (Pair for Siamese Network) Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ee2441d",
      "metadata": {
        "id": "8ee2441d",
        "papermill": {
          "duration": 0.003254,
          "end_time": "2025-06-30T03:01:27.915588",
          "exception": false,
          "start_time": "2025-06-30T03:01:27.912334",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Pairs formed (for both +ve and -ve)\n",
        "- Normal-Normal\n",
        "- Normal-Distorted\n",
        "- Distorted-Distorted\n",
        "\n",
        "\n",
        "All possible +ve pairs formed then -ve pairs formed accordingly to make balanced dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94b8b427",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-30T03:01:27.923650Z",
          "iopub.status.busy": "2025-06-30T03:01:27.923261Z",
          "iopub.status.idle": "2025-06-30T03:02:17.732747Z",
          "shell.execute_reply": "2025-06-30T03:02:17.731955Z"
        },
        "id": "94b8b427",
        "papermill": {
          "duration": 49.815091,
          "end_time": "2025-06-30T03:02:17.734077",
          "exception": false,
          "start_time": "2025-06-30T03:01:27.918986",
          "status": "completed"
        },
        "tags": [],
        "outputId": "26dcfb19-a9c0-4fbc-dd75-8fdf4af5740b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating positive pairs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 877/877 [00:22<00:00, 39.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total positive pairs: 34016\n",
            "Generating balanced negative pairs...\n",
            "Total pairs saved: 68032 (Positive: 34016, Negative: 34016)\n",
            "Generating positive pairs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 250/250 [00:04<00:00, 50.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total positive pairs: 619\n",
            "Generating balanced negative pairs...\n",
            "Total pairs saved: 1238 (Positive: 619, Negative: 619)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def load_images_from_folder(folder, image_size):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img_path = os.path.join(folder, filename)\n",
        "        if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):  # Ensure only images\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is not None:\n",
        "                img = cv2.resize(img, image_size)\n",
        "                images.append(img)\n",
        "    return images\n",
        "\n",
        "def create_pairs_with_distortions_balanced(base_path, save_dir, image_size=(128, 128)):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(save_dir, \"x1\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(save_dir, \"x2\"), exist_ok=True)\n",
        "\n",
        "    people = [p for p in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, p))]\n",
        "    pair_records = []\n",
        "    pair_id = 0\n",
        "    positive_pairs = []\n",
        "\n",
        "    print(\"Generating positive pairs...\")\n",
        "\n",
        "    # positive pairs\n",
        "    person_to_images = {}\n",
        "    for person in tqdm(people):\n",
        "        person_path = os.path.join(base_path, person)\n",
        "        normal_images = load_images_from_folder(person_path, image_size)\n",
        "        distorted_path = os.path.join(person_path, \"distorted\")\n",
        "        distorted_images = []\n",
        "        if os.path.exists(distorted_path):\n",
        "            distorted_images = load_images_from_folder(distorted_path, image_size)\n",
        "\n",
        "        all_images = normal_images + distorted_images\n",
        "        person_to_images[person] = all_images\n",
        "\n",
        "        for img1, img2 in combinations(all_images, 2):\n",
        "            positive_pairs.append((img1, img2))\n",
        "\n",
        "    num_positive = len(positive_pairs)\n",
        "    print(f\"Total positive pairs: {num_positive}\")\n",
        "\n",
        "    #Save positive pairs\n",
        "    for img1, img2 in positive_pairs:\n",
        "        x1_path = os.path.join(\"x1\", f\"pair_{pair_id}.jpg\")\n",
        "        x2_path = os.path.join(\"x2\", f\"pair_{pair_id}.jpg\")\n",
        "        cv2.imwrite(os.path.join(save_dir, x1_path), img1)\n",
        "        cv2.imwrite(os.path.join(save_dir, x2_path), img2)\n",
        "        pair_records.append([x1_path, x2_path, 1])\n",
        "        pair_id += 1\n",
        "\n",
        "    # Generate negative pairs equal in number to positive pairs\n",
        "    print(\"Generating balanced negative pairs...\")\n",
        "    negative_pairs = []\n",
        "    attempts = 0\n",
        "    max_attempts = num_positive * 10\n",
        "\n",
        "    while len(negative_pairs) < num_positive and attempts < max_attempts:\n",
        "        person1, person2 = random.sample(people, 2)\n",
        "        imgs1 = person_to_images[person1]\n",
        "        imgs2 = person_to_images[person2]\n",
        "\n",
        "        if not imgs1 or not imgs2:\n",
        "            attempts += 1\n",
        "            continue\n",
        "\n",
        "        img1 = random.choice(imgs1)\n",
        "        img2 = random.choice(imgs2)\n",
        "\n",
        "        negative_pairs.append((img1, img2))\n",
        "        attempts += 1\n",
        "\n",
        "\n",
        "    for img1, img2 in negative_pairs:\n",
        "        x1_path = os.path.join(\"x1\", f\"pair_{pair_id}.jpg\")\n",
        "        x2_path = os.path.join(\"x2\", f\"pair_{pair_id}.jpg\")\n",
        "        cv2.imwrite(os.path.join(save_dir, x1_path), img1)\n",
        "        cv2.imwrite(os.path.join(save_dir, x2_path), img2)\n",
        "        pair_records.append([x1_path, x2_path, 0])\n",
        "        pair_id += 1\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(pair_records, columns=[\"img1\", \"img2\", \"label\"])\n",
        "    df.to_csv(os.path.join(save_dir, \"pairs_labels.csv\"), index=False)\n",
        "    print(f\"Total pairs saved: {len(df)} (Positive: {df['label'].sum()}, Negative: {(df['label']==0).sum()})\")\n",
        "\n",
        "\n",
        "create_pairs_with_distortions_balanced(\n",
        "    \"/kaggle/input/comys-hackathon5-2025/Comys_Hackathon5/Task_B/train\",\n",
        "    \"/kaggle/working/train_pairs\"\n",
        ")\n",
        "\n",
        "create_pairs_with_distortions_balanced(\n",
        "    \"/kaggle/input/comys-hackathon5-2025/Comys_Hackathon5/Task_B/val\",\n",
        "    \"/kaggle/working/val_pairs\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "248fadd6",
      "metadata": {
        "id": "248fadd6",
        "papermill": {
          "duration": 0.009752,
          "end_time": "2025-06-30T03:02:17.754327",
          "exception": false,
          "start_time": "2025-06-30T03:02:17.744575",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89378e17",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-30T03:02:17.775178Z",
          "iopub.status.busy": "2025-06-30T03:02:17.774940Z",
          "iopub.status.idle": "2025-06-30T03:02:19.440217Z",
          "shell.execute_reply": "2025-06-30T03:02:19.439416Z"
        },
        "id": "89378e17",
        "papermill": {
          "duration": 1.676881,
          "end_time": "2025-06-30T03:02:19.441355",
          "exception": false,
          "start_time": "2025-06-30T03:02:17.764474",
          "status": "completed"
        },
        "tags": [],
        "outputId": "4dc41236-98ce-4b2c-b559-fa44d855694d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "class FacePairsDataset(Dataset):\n",
        "    def __init__(self, csv_path, base_dir, image_size=(128,128), augment=False):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.base_dir = base_dir\n",
        "        self.image_size = image_size\n",
        "        self.augment = augment\n",
        "\n",
        "        self.transform = A.Compose([\n",
        "            A.Resize(*image_size),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.Rotate(limit=15, p=0.5),\n",
        "            A.Normalize(),\n",
        "            ToTensorV2()\n",
        "                ]) if augment else A.Compose([\n",
        "                    A.Resize(*image_size),\n",
        "                    A.Normalize(),\n",
        "                    ToTensorV2()\n",
        "                ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img1_path = os.path.join(self.base_dir, row['img1'])\n",
        "        img2_path = os.path.join(self.base_dir, row['img2'])\n",
        "\n",
        "        img1 = cv2.imread(img1_path)\n",
        "        img2 = cv2.imread(img2_path)\n",
        "\n",
        "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
        "        img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        img1 = self.transform(image=img1)['image']\n",
        "        img2 = self.transform(image=img2)['image']\n",
        "\n",
        "        label = torch.tensor(row['label'], dtype=torch.float32)\n",
        "\n",
        "        return img1, img2, label\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d44fb8b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-30T03:02:19.463244Z",
          "iopub.status.busy": "2025-06-30T03:02:19.462670Z",
          "iopub.status.idle": "2025-06-30T03:02:19.535487Z",
          "shell.execute_reply": "2025-06-30T03:02:19.534690Z"
        },
        "id": "2d44fb8b",
        "papermill": {
          "duration": 0.084909,
          "end_time": "2025-06-30T03:02:19.536849",
          "exception": false,
          "start_time": "2025-06-30T03:02:19.451940",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_dataset = FacePairsDataset(\n",
        "    \"/kaggle/working/train_pairs/pairs_labels.csv\",\n",
        "    \"/kaggle/working/train_pairs\",\n",
        "    image_size=(128, 128),\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "val_dataset = FacePairsDataset(\n",
        "    \"/kaggle/working/val_pairs/pairs_labels.csv\",\n",
        "    \"/kaggle/working/val_pairs\",\n",
        "    image_size=(128, 128),\n",
        "    augment=False\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "318bbd10",
      "metadata": {
        "id": "318bbd10",
        "papermill": {
          "duration": 0.009809,
          "end_time": "2025-06-30T03:02:19.557295",
          "exception": false,
          "start_time": "2025-06-30T03:02:19.547486",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faec2d28",
      "metadata": {
        "id": "faec2d28",
        "papermill": {
          "duration": 0.009561,
          "end_time": "2025-06-30T03:02:19.576779",
          "exception": false,
          "start_time": "2025-06-30T03:02:19.567218",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Model Explanation: MSFF_WinAttn_MobileNet_Embedding\n",
        "\n",
        "This model is designed to produce **compact embedding vectors** from input images, suitable for applications like:\n",
        "\n",
        " Face or object embedding  \n",
        " Image retrieval  \n",
        " Metric learning\n",
        "\n",
        "It combines the strength of a pretrained **MobileNetV2** backbone with additional feature fusion and attention mechanisms.\n",
        "\n",
        "---\n",
        "\n",
        "##  **Model Components**\n",
        "\n",
        "###  1. MobileNetV2 Backbone\n",
        "- The pretrained MobileNetV2 network is split into 4 sequential stages:\n",
        "  - **Stage 1**: Early low-level features.\n",
        "  - **Stage 2**: Intermediate features.\n",
        "  - **Stage 3**: Deeper semantic features.\n",
        "  - **Stage 4**: High-level features.\n",
        "- Each stage produces feature maps of different resolution and semantics.\n",
        "\n",
        "---\n",
        "\n",
        "###  2. Feature Reduction\n",
        "- Each stage output is reduced to **256 channels** using a `1×1` convolution.\n",
        "- This ensures a consistent dimension across all stages before fusion.\n",
        "\n",
        "---\n",
        "\n",
        "###  3. Global Pooling & Dropout\n",
        "- Each reduced feature map is **globally average pooled** to a vector of shape `(batch_size, 256)`.\n",
        "- Dropout regularization is applied to prevent overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "###  4. Multi-Stage Feature Fusion (MSFF)\n",
        "- The four pooled vectors are **stacked along a new dimension**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f94166b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-30T03:02:19.597306Z",
          "iopub.status.busy": "2025-06-30T03:02:19.597074Z",
          "iopub.status.idle": "2025-06-30T03:02:19.610624Z",
          "shell.execute_reply": "2025-06-30T03:02:19.609916Z"
        },
        "id": "8f94166b",
        "papermill": {
          "duration": 0.025263,
          "end_time": "2025-06-30T03:02:19.611766",
          "exception": false,
          "start_time": "2025-06-30T03:02:19.586503",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob=0.):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.drop_prob == 0. or not self.training:\n",
        "            return x\n",
        "        keep_prob = 1 - self.drop_prob\n",
        "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "        random_tensor.floor_()\n",
        "        return x.div(keep_prob) * random_tensor\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = (dim // heads) ** -0.5\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(\n",
        "            lambda t: t.view(B, N, self.heads, C // self.heads).transpose(1, 2),\n",
        "            qkv\n",
        "        )\n",
        "        dots = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "        attn = dots.softmax(dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).reshape(B, N, C)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class MSFF_WinAttn_MobileNet_Embedding(nn.Module):\n",
        "    def __init__(self, embedding_dim=128, drop_path_prob=0.1):\n",
        "        super().__init__()\n",
        "        mobilenet = models.mobilenet_v2(pretrained=True).features\n",
        "\n",
        "        self.stage1 = mobilenet[:4]\n",
        "        self.stage2 = mobilenet[4:7]\n",
        "        self.stage3 = mobilenet[7:14]\n",
        "        self.stage4 = mobilenet[14:]\n",
        "\n",
        "        self.reduce1 = nn.Conv2d(24, 256, 1)\n",
        "        self.reduce2 = nn.Conv2d(32, 256, 1)\n",
        "        self.reduce3 = nn.Conv2d(96, 256, 1)\n",
        "        self.reduce4 = nn.Conv2d(1280, 256, 1)\n",
        "\n",
        "        self.attn1 = WindowAttention(256, heads=4)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.drop_path1 = DropPath(drop_path_prob)\n",
        "\n",
        "        self.attn2 = WindowAttention(256, heads=4)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.drop_path2 = DropPath(drop_path_prob)\n",
        "\n",
        "        self.embed_fc = nn.Linear(256 * 4, embedding_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.bn_final = nn.BatchNorm1d(256 * 4)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.stage1(x)\n",
        "        x2 = self.stage2(x1)\n",
        "        x3 = self.stage3(x2)\n",
        "        x4 = self.stage4(x3)\n",
        "\n",
        "        x1 = self.dropout(F.adaptive_avg_pool2d(self.reduce1(x1), 1).flatten(1))\n",
        "        x2 = self.dropout(F.adaptive_avg_pool2d(self.reduce2(x2), 1).flatten(1))\n",
        "        x3 = self.dropout(F.adaptive_avg_pool2d(self.reduce3(x3), 1).flatten(1))\n",
        "        x4 = self.dropout(F.adaptive_avg_pool2d(self.reduce4(x4), 1).flatten(1))\n",
        "\n",
        "\n",
        "        feats = torch.stack([x1, x2, x3, x4], dim=1)\n",
        "        feats = self.attn1(feats)\n",
        "        feats = self.drop_path1(feats)\n",
        "        B, N, C = feats.shape\n",
        "        feats = feats.view(B * N, C)\n",
        "        feats = self.bn1(feats)\n",
        "        feats = F.relu(feats)\n",
        "        feats = feats.view(B, N, C)\n",
        "\n",
        "        feats = self.attn2(feats)\n",
        "        feats = self.drop_path2(feats)\n",
        "        feats = feats.view(B * N, C)\n",
        "        feats = self.bn2(feats)\n",
        "        feats = F.relu(feats)\n",
        "        feats = feats.view(B, N, C)\n",
        "\n",
        "        out = feats.flatten(1)\n",
        "        # embed = self.embed_fc(out)\n",
        "        out = self.dropout(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.bn_final(out)\n",
        "        embed = self.embed_fc(out)\n",
        "\n",
        "        embed = F.normalize(embed, p=2, dim=1)\n",
        "        return embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0654eb10",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-30T03:02:19.633049Z",
          "iopub.status.busy": "2025-06-30T03:02:19.632851Z",
          "iopub.status.idle": "2025-06-30T03:02:19.637022Z",
          "shell.execute_reply": "2025-06-30T03:02:19.636336Z"
        },
        "id": "0654eb10",
        "papermill": {
          "duration": 0.016233,
          "end_time": "2025-06-30T03:02:19.638062",
          "exception": false,
          "start_time": "2025-06-30T03:02:19.621829",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super().__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, emb1, emb2, label):\n",
        "        dist = F.pairwise_distance(emb1, emb2)\n",
        "        loss_same = label * dist.pow(2)\n",
        "        loss_diff = (1 - label) * F.relu(self.margin - dist).pow(2)\n",
        "        return 0.5 * (loss_same + loss_diff).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cea77ca6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-30T03:02:19.658968Z",
          "iopub.status.busy": "2025-06-30T03:02:19.658754Z",
          "iopub.status.idle": "2025-06-30T03:02:20.435068Z",
          "shell.execute_reply": "2025-06-30T03:02:20.434479Z"
        },
        "id": "cea77ca6",
        "papermill": {
          "duration": 0.788267,
          "end_time": "2025-06-30T03:02:20.436371",
          "exception": false,
          "start_time": "2025-06-30T03:02:19.648104",
          "status": "completed"
        },
        "tags": [],
        "outputId": "d6df5b30-6eb9-4168-880f-6ff1ca45ce51"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 117MB/s] \n"
          ]
        }
      ],
      "source": [
        "model = MSFF_WinAttn_MobileNet_Embedding(embedding_dim=128).cuda()\n",
        "criterion = ContrastiveLoss(margin=1.0)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "num_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46a766f5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-30T03:02:20.457976Z",
          "iopub.status.busy": "2025-06-30T03:02:20.457745Z",
          "iopub.status.idle": "2025-06-30T03:02:20.567397Z",
          "shell.execute_reply": "2025-06-30T03:02:20.566512Z"
        },
        "id": "46a766f5",
        "papermill": {
          "duration": 0.1217,
          "end_time": "2025-06-30T03:02:20.568668",
          "exception": false,
          "start_time": "2025-06-30T03:02:20.446968",
          "status": "completed"
        },
        "tags": [],
        "outputId": "c14287b8-01c6-48fe-a22a-d306bdbf3df3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = MSFF_WinAttn_MobileNet_Embedding(embedding_dim=128)\n",
        "\n",
        "\n",
        "\n",
        "model = model.cuda()\n",
        "criterion = ContrastiveLoss(margin=1.0)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "\n",
        "num_epochs = 30\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "741d6778",
      "metadata": {
        "id": "741d6778",
        "papermill": {
          "duration": 0.010868,
          "end_time": "2025-06-30T03:02:20.590246",
          "exception": false,
          "start_time": "2025-06-30T03:02:20.579378",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "629174a9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-30T03:02:20.613258Z",
          "iopub.status.busy": "2025-06-30T03:02:20.613020Z",
          "iopub.status.idle": "2025-06-30T04:51:57.905172Z",
          "shell.execute_reply": "2025-06-30T04:51:57.903999Z"
        },
        "id": "629174a9",
        "papermill": {
          "duration": 6577.305646,
          "end_time": "2025-06-30T04:51:57.906408",
          "exception": false,
          "start_time": "2025-06-30T03:02:20.600762",
          "status": "completed"
        },
        "tags": [],
        "outputId": "5967b71b-404c-4287-d3b0-6d70182c3a74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30 | Train Loss: 0.0573 | Train Acc: 0.9010 | Train F1: 0.9010 | Val Loss: 0.1178 | Val Acc: 0.6866 | Val F1: 0.6717\n",
            "\n",
            "Epoch 2/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/30 | Train Loss: 0.0263 | Train Acc: 0.9481 | Train F1: 0.9481 | Val Loss: 0.0865 | Val Acc: 0.7544 | Val F1: 0.7508\n",
            "\n",
            "Epoch 3/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/30 | Train Loss: 0.0193 | Train Acc: 0.9567 | Train F1: 0.9567 | Val Loss: 0.1077 | Val Acc: 0.6535 | Val F1: 0.6282\n",
            "\n",
            "Epoch 4/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/30 | Train Loss: 0.0176 | Train Acc: 0.9592 | Train F1: 0.9592 | Val Loss: 0.0870 | Val Acc: 0.7399 | Val F1: 0.7320\n",
            "\n",
            "Epoch 5/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/30 | Train Loss: 0.0164 | Train Acc: 0.9627 | Train F1: 0.9627 | Val Loss: 0.1384 | Val Acc: 0.6397 | Val F1: 0.6092\n",
            "\n",
            "Epoch 6/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/30 | Train Loss: 0.0162 | Train Acc: 0.9625 | Train F1: 0.9625 | Val Loss: 0.1589 | Val Acc: 0.6470 | Val F1: 0.6146\n",
            "\n",
            "Epoch 7/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/30 | Train Loss: 0.0134 | Train Acc: 0.9689 | Train F1: 0.9689 | Val Loss: 0.0872 | Val Acc: 0.7342 | Val F1: 0.7263\n",
            "\n",
            "Epoch 8/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/30 | Train Loss: 0.0128 | Train Acc: 0.9694 | Train F1: 0.9694 | Val Loss: 0.1004 | Val Acc: 0.6995 | Val F1: 0.6838\n",
            "\n",
            "Epoch 9/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/30 | Train Loss: 0.0126 | Train Acc: 0.9697 | Train F1: 0.9697 | Val Loss: 0.0980 | Val Acc: 0.7141 | Val F1: 0.7015\n",
            "\n",
            "Epoch 10/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/30 | Train Loss: 0.0121 | Train Acc: 0.9711 | Train F1: 0.9711 | Val Loss: 0.0972 | Val Acc: 0.7084 | Val F1: 0.6911\n",
            "\n",
            "Epoch 11/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/30 | Train Loss: 0.0121 | Train Acc: 0.9708 | Train F1: 0.9708 | Val Loss: 0.0803 | Val Acc: 0.7577 | Val F1: 0.7517\n",
            "\n",
            "Epoch 12/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/30 | Train Loss: 0.0117 | Train Acc: 0.9717 | Train F1: 0.9717 | Val Loss: 0.0851 | Val Acc: 0.7399 | Val F1: 0.7311\n",
            "\n",
            "Epoch 13/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/30 | Train Loss: 0.0119 | Train Acc: 0.9711 | Train F1: 0.9711 | Val Loss: 0.0826 | Val Acc: 0.7423 | Val F1: 0.7343\n",
            "\n",
            "Epoch 14/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/30 | Train Loss: 0.0117 | Train Acc: 0.9717 | Train F1: 0.9717 | Val Loss: 0.0904 | Val Acc: 0.7221 | Val F1: 0.7087\n",
            "\n",
            "Epoch 15/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/30 | Train Loss: 0.0119 | Train Acc: 0.9709 | Train F1: 0.9708 | Val Loss: 0.0891 | Val Acc: 0.7367 | Val F1: 0.7257\n",
            "\n",
            "Epoch 16/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/30 | Train Loss: 0.0117 | Train Acc: 0.9717 | Train F1: 0.9717 | Val Loss: 0.0893 | Val Acc: 0.7246 | Val F1: 0.7126\n",
            "\n",
            "Epoch 17/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/30 | Train Loss: 0.0117 | Train Acc: 0.9719 | Train F1: 0.9719 | Val Loss: 0.0916 | Val Acc: 0.7254 | Val F1: 0.7125\n",
            "\n",
            "Epoch 18/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/30 | Train Loss: 0.0116 | Train Acc: 0.9725 | Train F1: 0.9725 | Val Loss: 0.1000 | Val Acc: 0.6987 | Val F1: 0.6806\n",
            "\n",
            "Epoch 19/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/30 | Train Loss: 0.0117 | Train Acc: 0.9718 | Train F1: 0.9718 | Val Loss: 0.0838 | Val Acc: 0.7520 | Val F1: 0.7443\n",
            "\n",
            "Epoch 20/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/30 | Train Loss: 0.0117 | Train Acc: 0.9716 | Train F1: 0.9716 | Val Loss: 0.0933 | Val Acc: 0.7132 | Val F1: 0.6999\n",
            "\n",
            "Epoch 21/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21/30 | Train Loss: 0.0119 | Train Acc: 0.9711 | Train F1: 0.9711 | Val Loss: 0.0841 | Val Acc: 0.7423 | Val F1: 0.7348\n",
            "\n",
            "Epoch 22/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22/30 | Train Loss: 0.0115 | Train Acc: 0.9725 | Train F1: 0.9725 | Val Loss: 0.0899 | Val Acc: 0.7318 | Val F1: 0.7204\n",
            "\n",
            "Epoch 23/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23/30 | Train Loss: 0.0113 | Train Acc: 0.9727 | Train F1: 0.9727 | Val Loss: 0.0899 | Val Acc: 0.7221 | Val F1: 0.7095\n",
            "\n",
            "Epoch 24/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24/30 | Train Loss: 0.0119 | Train Acc: 0.9709 | Train F1: 0.9709 | Val Loss: 0.0845 | Val Acc: 0.7447 | Val F1: 0.7361\n",
            "\n",
            "Epoch 25/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25/30 | Train Loss: 0.0117 | Train Acc: 0.9722 | Train F1: 0.9722 | Val Loss: 0.0893 | Val Acc: 0.7286 | Val F1: 0.7178\n",
            "\n",
            "Epoch 26/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26/30 | Train Loss: 0.0118 | Train Acc: 0.9717 | Train F1: 0.9717 | Val Loss: 0.0913 | Val Acc: 0.7270 | Val F1: 0.7152\n",
            "\n",
            "Epoch 27/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27/30 | Train Loss: 0.0116 | Train Acc: 0.9719 | Train F1: 0.9719 | Val Loss: 0.0829 | Val Acc: 0.7472 | Val F1: 0.7387\n",
            "\n",
            "Epoch 28/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28/30 | Train Loss: 0.0119 | Train Acc: 0.9710 | Train F1: 0.9710 | Val Loss: 0.0860 | Val Acc: 0.7407 | Val F1: 0.7322\n",
            "\n",
            "Epoch 29/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29/30 | Train Loss: 0.0116 | Train Acc: 0.9723 | Train F1: 0.9723 | Val Loss: 0.0843 | Val Acc: 0.7431 | Val F1: 0.7355\n",
            "\n",
            "Epoch 30/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30/30 | Train Loss: 0.0115 | Train Acc: 0.9717 | Train F1: 0.9717 | Val Loss: 0.0979 | Val Acc: 0.7108 | Val F1: 0.6966\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "best_val_loss = None\n",
        "threshold = 0.5  # Distance threshold\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    all_train_preds = []\n",
        "    all_train_labels = []\n",
        "    train_batches = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for img1, img2, labels in train_batches:\n",
        "        img1 = img1.cuda()\n",
        "        img2 = img2.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        emb1 = model(img1)\n",
        "        emb2 = model(img2)\n",
        "\n",
        "        loss = criterion(emb1, emb2, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_batches.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Collect predictions for metrics\n",
        "        dists = F.pairwise_distance(emb1, emb2)\n",
        "        preds = (dists < threshold).long()\n",
        "        all_train_preds.extend(preds.cpu().numpy())\n",
        "        all_train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    all_train_preds = np.array(all_train_preds)\n",
        "    all_train_labels = np.array(all_train_labels)\n",
        "    train_accuracy = (all_train_preds == all_train_labels).mean()\n",
        "    train_f1 = f1_score(all_train_labels, all_train_preds, average=\"macro\")\n",
        "\n",
        "    #Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "    val_batches = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img1, img2, labels in val_batches:\n",
        "            img1 = img1.cuda()\n",
        "            img2 = img2.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            emb1 = model(img1)\n",
        "            emb2 = model(img2)\n",
        "\n",
        "            loss = criterion(emb1, emb2, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            dists = F.pairwise_distance(emb1, emb2)\n",
        "            preds = (dists < threshold).long()\n",
        "\n",
        "            all_val_preds.extend(preds.cpu().numpy())\n",
        "            all_val_labels.extend(labels.cpu().numpy())\n",
        "            val_batches.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    all_val_preds = np.array(all_val_preds)\n",
        "    all_val_labels = np.array(all_val_labels)\n",
        "    val_accuracy = (all_val_preds == all_val_labels).mean()\n",
        "    val_f1 = f1_score(all_val_labels, all_val_preds, average=\"macro\")\n",
        "\n",
        "    # Save best model\n",
        "    if best_val_loss is None or avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1}/{num_epochs} \"\n",
        "        f\"| Train Loss: {avg_train_loss:.4f} \"\n",
        "        f\"| Train Acc: {train_accuracy:.4f} \"\n",
        "        f\"| Train F1: {train_f1:.4f} \"\n",
        "        f\"| Val Loss: {avg_val_loss:.4f} \"\n",
        "        f\"| Val Acc: {val_accuracy:.4f} \"\n",
        "        f\"| Val F1: {val_f1:.4f}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66b149f2",
      "metadata": {
        "id": "66b149f2",
        "papermill": {
          "duration": 4.872092,
          "end_time": "2025-06-30T04:52:07.811087",
          "exception": false,
          "start_time": "2025-06-30T04:52:02.938995",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Testing Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f256d628",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-30T04:52:17.496203Z",
          "iopub.status.busy": "2025-06-30T04:52:17.495905Z",
          "iopub.status.idle": "2025-06-30T04:52:17.504357Z",
          "shell.execute_reply": "2025-06-30T04:52:17.503819Z"
        },
        "id": "f256d628",
        "papermill": {
          "duration": 4.860513,
          "end_time": "2025-06-30T04:52:17.505444",
          "exception": false,
          "start_time": "2025-06-30T04:52:12.644931",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def test_model(model, model_path, test_folder, device='cuda' if torch.cuda.is_available() else 'cpu', batch_size=32):\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    create_pairs_with_distortions_balanced(test_folder,\"./test_pairs\")\n",
        "    test_dataset = FacePairsDataset(\"./test_pairs/pairs_labels.csv\",\"./test_pairs\",image_size=(128, 128),augment=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    val_loss = 0.0\n",
        "    test_batches = tqdm(test_loader, desc=\"test\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img1, img2, labels in test_batches:\n",
        "            img1 = img1.cuda()\n",
        "            img2 = img2.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            emb1 = model(img1)\n",
        "            emb2 = model(img2)\n",
        "\n",
        "            loss = criterion(emb1, emb2, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            dists = F.pairwise_distance(emb1, emb2)\n",
        "            preds = (dists < threshold).long()\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    rec = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"\\n Test Accuracy: {acc:.4f}\")\n",
        "    print(f\" Precision:     {prec:.4f}\")\n",
        "    print(f\" Recall:        {rec:.4f}\")\n",
        "    print(f\" F1 Score:      {f1:.4f}\")\n",
        "\n",
        "    return acc, prec, rec, f1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a333e302",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-30T04:52:27.141413Z",
          "iopub.status.busy": "2025-06-30T04:52:27.140963Z",
          "iopub.status.idle": "2025-06-30T04:52:32.328519Z",
          "shell.execute_reply": "2025-06-30T04:52:32.327464Z"
        },
        "id": "a333e302",
        "papermill": {
          "duration": 10.039458,
          "end_time": "2025-06-30T04:52:32.329776",
          "exception": false,
          "start_time": "2025-06-30T04:52:22.290318",
          "status": "completed"
        },
        "tags": [],
        "outputId": "d7449164-d5ff-44b3-850f-925563f84e03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating positive pairs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 250/250 [00:02<00:00, 90.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total positive pairs: 619\n",
            "Generating balanced negative pairs...\n",
            "Total pairs saved: 1238 (Positive: 619, Negative: 619)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                     "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Test Accuracy: 0.7593\n",
            " Precision:     0.7850\n",
            " Recall:        0.7593\n",
            " F1 Score:      0.7537\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "test_folder = \"/kaggle/input/comys-hackathon5-2025/Comys_Hackathon5/Task_B/val\"\n",
        "model_path = \"/kaggle/working/best_model.pth\"\n",
        "\n",
        "acc, prec, rec, f1 = test_model(model, model_path, test_folder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_folder = \"/content/extracted_folder/Comys_Hackathon5/Task_B/train\"\n",
        "model_path = \"/content/best_model.pth\"\n",
        "\n",
        "acc, prec, rec, f1 = test_model(model, model_path, test_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLAsjP7jvZ7y",
        "outputId": "11e2bca6-e799-4049-a2f8-567f53b87a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating positive pairs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 877/877 [00:06<00:00, 131.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total positive pairs: 34016\n",
            "Generating balanced negative pairs...\n",
            "Total pairs saved: 68032 (Positive: 34016, Negative: 34016)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Test Accuracy: 0.9745\n",
            " Precision:     0.9755\n",
            " Recall:        0.9745\n",
            " F1 Score:      0.9745\n"
          ]
        }
      ],
      "id": "tLAsjP7jvZ7y"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 7766818,
          "sourceId": 12321727,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 6699.481787,
      "end_time": "2025-06-30T04:52:50.171623",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-06-30T03:01:10.689836",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}